{
  "metadata" : {
    "name" : "Just Enough Scala for Spark",
    "user_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T01:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "09F311AC8FF547A584ADF2C515CAA324"
    },
    "cell_type" : "code",
    "source" : "println(\"Hello World!\")  // Our first Scala program, and a sanity check",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Hello World!\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2,
      "time" : "Took: 2 seconds 122 milliseconds, at 2017-5-22 15:52"
    } ]
  }, {
    "metadata" : {
      "id" : "DB7A35849BCB4167802DB59D5CA868E8"
    },
    "cell_type" : "markdown",
    "source" : "# Just Enough Scala for Spark\n\n[Dean Wampler, Ph.D.](mailto:deanwampler@gmail.com)<br/>\n[@deanwampler](http://twitter.com/deanwampler)<br/>\n[Lightbend Fast Data Platform](http://lightbend.com/fast-data-platform)\n\nWelcome. This notebook teaches you the core concepts of [Scala](http://scala-lang.org) necessary to use [Apache Spark's](http://spark.apache.org) Scala API effectively. Spark does a nice job exploiting the nicest features of Scala, while avoiding most of the more difficult and obscure features. "
  }, {
    "metadata" : {
      "id" : "FEB41A56E21F408A966052A75B819C8E"
    },
    "cell_type" : "markdown",
    "source" : "## Introduction: Why Scala?\nSpark lets you use Scala, Java, Python, R, and SQL to do your work. Scala and Java appeal to _data engineers_, who do the heavy lifting of building resilient and scalable infrastructures for _Big Data_. Python, R, and SQL appeal to _data scientists_, who build models for analyzing data, including machine learning, as well as explore data interactively, where SQL is very convenient.\n\nThese aren't hard boundaries. Many people do both roles. Many data engineers like Python and may use SQL and R. Many data scientists have decided to use Scala with Spark.\n\nBriefly, some of the advantages of using Scala include the following:\n* **Performance:** Since Spark is written in Scala, when you use the [RDD](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds) API, you get the best performance and the most complete API coverage when you use Scala. However, with [DataFrames](http://spark.apache.org/docs/latest/sql-programming-guide.html), code written in all five languages performs about the same.\n* **Debugging:** When runtime problems occur, understanding the exception stack trace and other debug information is easiest if you know Scala. Unfortunately, the abstractions provided by the different language APIs \"leak\" when problems occur.\n* **Concise, Expressive Code:** Compared to Java, Scala code is much more concise and several features of Scala make your code even more concise. This elevates your productivity and makes it easier to imagine a design approach and then write it down without having to translate the idea to a less flexible API that reflects idiomatic language constraints. (You'll see this in action as we go.)\n* **Type Safety:** Compared to Python and R, Scala code benefits from _static typing_ with _type inference_. _Static typing_ means that the Scala parser finds more errors in your expressions at compile time, when they don't match expected types, rather than discovering the problem later at run time. However, _type inference_ means you don't have to add a lot of explicit type information to you code. In most cases, Scala will infer the correct types for you."
  }, {
    "metadata" : {
      "id" : "643793D69BF84D788D14E50DBE1CED14"
    },
    "cell_type" : "markdown",
    "source" : "### Why Not Scala?\nScala isn't perfect. There are two disadvantages compared to Python and R:\n* **Libraries:** Python and R have a rich ecosystem of data analytics libraries. While the picture is improving for Scala, Python and R are still well ahead.\n* **Advanced Language Features:** Mastering advanced language features gives you a lot of power to exploit, but if you don't understand those features, they can get in your way when you're just trying to get work done. Scala has some sophisticated constructs, especially in its _type system_. Fortunately, Spark mostly hides the advanced constructs."
  }, {
    "metadata" : {
      "id" : "1D3A90B8EB19432E9D569E0BDF099FA8"
    },
    "cell_type" : "markdown",
    "source" : "### For More on Scala\nI can only scratch the surface of Scala here. We'll \"sketch\" many concepts without too much depth. You'll drink from a firehose, but learn enough to make use of the concepts. Eventually, when you're ready to deepen your understanding, consider these resources:\n\n* [Programming Scala, Second Edition](http://shop.oreilly.com/product/0636920033073.do): My comprehensive introduction to Scala.\n* [Scala Language Website](http://scala-lang.org/): Where to download Scala, find documentation (e.g., the [Scaladocs](http://www.scala-lang.org/api/current/#package): Scala library documentation, like [Javadocs](https://docs.oracle.com/javase/8/docs/api/)), and other information.\n* [Lightbend](http://www.lightbend.com/services/) training, consulting, and support for Scala, Big Data tools like Spark, and the [Lightbend Reactive Platform](http://www.lightbend.com/products/lightbend-reactive-platform).\n\nFor now, I recommend that you open the Scaladocs for Scala and for Spark's Scala API. Clicking these two links will open them in new browser tabs:\n* Scaladocs for <a href=\"http://www.scala-lang.org/api/2.11.8/#package\" target=\"scala_scaladocs\">Scala</a>.\n* Scaladocs for <a href=\"http://spark.apache.org/docs/latest/api/scala/index.html#package\" target=\"spark_scaladocs\">Spark</a>."
  }, {
    "metadata" : {
      "id" : "09039ABE472F4B09AA5969B8FAA7E2BD"
    },
    "cell_type" : "markdown",
    "source" : "> **Tips for using Scaladocs:**\n* Use the search bar in the upper-left-hand side to find a particular _type_. (For example, try \"RDD\" in the Spark Scaladocs.) \n* To search for a particular _method_, click the character under the search box for the method name's first letter, then scroll to it."
  }, {
    "metadata" : {
      "id" : "40AA3A2B596F44CBB4955A0F282E6D28"
    },
    "cell_type" : "markdown",
    "source" : "## About Notebooks\nYou're using the [Spark Notebook](http://spark-notebook.io/) environment, a Scala-centric fork of [iPython](https://ipython.org/) configured for [Apache Spark](http://spark.apache.org).\n\nNotebooks let you mix documentation, like this [Markdown](https://daringfireball.net/projects/markdown/) \"cell\", with cells that contain code, graphs of results, etc. The metaphor is a physical notebook a scientist or student might use while working in a laboratory.\n\nThe menus and toolbar at the top provide options for evaluating a cell, adding and deleting cells, etc. You'll want to learn keyboard shortcuts if you use notebooks a lot.\n\nLet's write our second Scala program. We won't explain the syntax now, but is it easy enough to understand?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "346547D6A16E4CAEA6A208545E4F593B"
    },
    "cell_type" : "code",
    "source" : "(1 to 5).foreach(i => println(\"==> \"+i))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "==> 1\n==> 2\n==> 3\n==> 4\n==> 5\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 3,
      "time" : "Took: 2 seconds 356 milliseconds, at 2017-5-22 15:52"
    } ]
  }, {
    "metadata" : {
      "id" : "5A1E9EC40F5A423A8C42B542CD7D6378"
    },
    "cell_type" : "markdown",
    "source" : "> **Tips:**\n\n> Invoke the _Help > Keyboard Shortcuts_ menu item, then capture the page as an image (it's a modal dialog, unfortunately). Learn a few shortcuts each day.\n\n> For now, just know that you can click into any cell to move the focus. When you're in a cell, `shift+return` evaluates the cell (parses and renders the Markdown or runs the code), then moves to the next cell. Try it for a few cells. I'll wait...\n\n> Finally, there is a right-hand sidebar with useful information. If you want to view or hide the sidebar, use the _View > Toggle sidebar_ menu item. In the sidebar you'll see a link _open SparkUI_ to see Spark's own web UI. Use it to see more information about what your Spark jobs are doing."
  }, {
    "metadata" : {
      "id" : "E564A62836C7112AEB90CD6"
    },
    "cell_type" : "markdown",
    "source" : ">  **If You Are Using Docker...**\n\n> Unfortunately, this notebook will disappear, along with any edits you make to it, once the Docker image is shutdown. I recommend that you periodically use _File > Download as > Spark Notebook (.snb)_ to save your work!"
  }, {
    "metadata" : {
      "id" : "1085BD83DF844ADC87E5E73DC4F419B6"
    },
    "cell_type" : "markdown",
    "source" : "Okay. It's particularly nice that you can edit a cell you've already evaluated and rerun it. This is great when you're experimenting with code."
  }, {
    "metadata" : {
      "id" : "EC8003BB87D04FC68E133F9ED9B49A81"
    },
    "cell_type" : "markdown",
    "source" : "### `SparkSession` and `SparkContext`"
  }, {
    "metadata" : {
      "id" : "25BC8DA67F874D09836EE3400D876755"
    },
    "cell_type" : "markdown",
    "source" : "When you start this notebook, Spark Notebook creates a [SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession), the [entry point](http://spark.apache.org/docs/latest/programming-guide.html#initializing-spark) for Spark 2.X programs. Spark Notebook also creates a variable pointing to the older, Spark 1.X entry point, [SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext). If you are using Spark 1.X at work, this is the entry point you would use for your programs.\n\n`SparkSession` and `SparkContext` know how to connect to your cluster (or run locally in the same JVM, which is what we are doing today), how to configure properties, etc. They also run a Web UI that lets you monitor your running jobs. \n\nThe instance of `SparkSession` is called `sparkSession`. The instance of `SparkContext` is called `sc`. \n\nThe next two cells simply confirm that they exist."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6F2EF60A0E3A47B0962DB0003A5C9395"
    },
    "cell_type" : "code",
    "source" : "sparkSession",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res7: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4b3833ba\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.sql.SparkSession@4b3833ba"
      },
      "output_type" : "execute_result",
      "execution_count" : 4,
      "time" : "Took: 2 seconds 25 milliseconds, at 2017-5-22 16:34"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "EE12AC334C4F4DEA957BDC14070B3468"
    },
    "cell_type" : "code",
    "source" : "sc",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res9: org.apache.spark.SparkContext = org.apache.spark.SparkContext@30141f88\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.SparkContext@30141f88"
      },
      "output_type" : "execute_result",
      "execution_count" : 5,
      "time" : "Took: 1 second 689 milliseconds, at 2017-5-22 16:36"
    } ]
  }, {
    "metadata" : {
      "id" : "4BF7C09CA16145A78C7ACF46ADC3B522"
    },
    "cell_type" : "markdown",
    "source" : "Note that if you just evaluate a variable reference, it prints the value. More accurately, it prints the result of calling `toString` on the value. For `SparkSession` and `SparkContext`, that's a boring result; just the location in memory. This is what Java's default `toString` does for all objects. So, the Spark project doesn't provide custom implementations of `toString` for `SparkSession` and `SparkContext`"
  }, {
    "metadata" : {
      "id" : "D00483AC7F05429E9D2B02F50652BEE3"
    },
    "cell_type" : "markdown",
    "source" : "Actually, the `SparkContext` instance is really just a \"field\" in the `SparkSession` instance. Note that `sc` and `sparkSession.sparkContext` point to the same object:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3FF18DCC60524A5696F4FB796A588D40"
    },
    "cell_type" : "code",
    "source" : "sparkSession.sparkContext",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res11: org.apache.spark.SparkContext = org.apache.spark.SparkContext@30141f88\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "org.apache.spark.SparkContext@30141f88"
      },
      "output_type" : "execute_result",
      "execution_count" : 6,
      "time" : "Took: 1 second 236 milliseconds, at 2017-5-22 16:37"
    } ]
  }, {
    "metadata" : {
      "id" : "8C9131CF1E354A2CB9103CA66C6470E9"
    },
    "cell_type" : "markdown",
    "source" : "Here are a few other bits of information we can get from the `SparkContext`:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "02BE99E15B5245FBB07CE38840FC7DB4"
    },
    "cell_type" : "code",
    "source" : "println(\"Spark version:      \" + sc.version)\nprintln(\"Spark master:       \" + sc.master)\nprintln(\"Running 'locally'?: \" + sc.isLocal) // Again, \"locally\" means running everything in the one JVM ",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Spark version:      2.1.0\nSpark master:       local[*]\nRunning 'locally'?: true\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7,
      "time" : "Took: 1 second 455 milliseconds, at 2017-5-22 16:37"
    } ]
  }, {
    "metadata" : {
      "id" : "06B794609083404EB53553A5C98FF1E5"
    },
    "cell_type" : "markdown",
    "source" : "## Let's Download Some Data (and Start Learning Scala)\nWe're going to write real Spark programs and use them as vehicles for learning Scala and how to use it with Spark.\n\nBut first, we need to download some text files we'll use, which contain some of the plays of Shakespeare. The next few cells define some helper methods (functions) to do this and then perform the download. We'll start learning Scala concepts as we go."
  }, {
    "metadata" : {
      "id" : "D43407B76FEB47138B4C172B8686399A"
    },
    "cell_type" : "markdown",
    "source" : "> **Note:** \"method\" vs. \"function\"\n\n> Scala follows a common object-oriented convention where the term _method_ is used for a function that's attached to a class or instance. Scala also has _functions_ that are not associated with a particular class or instance. Java 8 introduced this idea of functions into Java, too, where they are usually called _lambdas_ (for historical reasons...) \n\n> In our next code example, we'll define a few helper _methods_ for printing information, but you won't see a class definition here. So, what class is associated with these methods? When you use Scala in a notebook, you're actually using the Scala interpreter, which wraps any expressions and definitions we write into a hidden, generated class. The interpreter has to do this in order to generate valid JVM byte code. \n\n> Unfortunately, it can be a bit confusing when to use a method vs. a function, reflecting Scala's hybrid nature as an object-oriented and a functional language. Fortunately, in many cases, we can use methods and functions interchangably, so we won't worry about the distinction too much from now on.\n\n> Again, we're defining _methods_ now. We'll see what a real _function_ looks like soon."
  }, {
    "metadata" : {
      "id" : "9EB0F4904E62411ABB4732FCE434F3E2"
    },
    "cell_type" : "markdown",
    "source" : "Okay, here are two convenience methods for printing either an error message or a simple \"information\" message. We'll explain all the syntax in a moment."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CEDDE3DA00584FD091761BB897BA76B8"
    },
    "cell_type" : "code",
    "source" : "/*\n * \"info\" takes a single String argument, prints it on a line,\n * and returns it. \n */\ndef info(message: String): String = {\n    println(message)\n\n    // The last expression in the block, message, is the return value. \n    // \"return\" keyword not required.\n    // Do no additional formatting for the return string.\n    message  // No additional formatting\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "info: (message: String)String\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8,
      "time" : "Took: 931 milliseconds, at 2017-5-22 16:40"
    } ]
  }, {
    "metadata" : {
      "id" : "5E2141A905EE470F8A3309CC21265968"
    },
    "cell_type" : "markdown",
    "source" : "Note how the signature of the method is written in the result, which is what the Scala interpreter returns. The return type of the method is the `String` at the end."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "10CDCBBDF9B0414381D29AA57001B4FE"
    },
    "cell_type" : "code",
    "source" : "/*\n * \"error\" takes a single String argument, prints a formatted error message,\n * and returns the message. \n */\ndef error(message: String): String = {   \n    \n    // Print the string passed to \"println\" and add a linefeed (\"ln\"):\n    // See the next cell for an explanation of how the string is constructed.\n    val fullMessage = s\"\"\"\n        |********************************************************************\n        |\n        |  ERROR: $message\n        |\n        |********************************************************************\n        |\"\"\".stripMargin\n    println(fullMessage)\n    \n    fullMessage\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "error: (message: String)String\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 9,
      "time" : "Took: 1 second 41 milliseconds, at 2017-5-22 16:41"
    } ]
  }, {
    "metadata" : {
      "id" : "803A3B24EDA44D8E8D0387961CED80C0"
    },
    "cell_type" : "markdown",
    "source" : "Let's try them:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "094980E7109040EA916AFE8EAA902C35"
    },
    "cell_type" : "code",
    "source" : "val infoString = info(\"All is well.\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "All is well.\ninfoString: String = All is well.\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 10,
      "time" : "Took: 1 second 221 milliseconds, at 2017-5-22 16:41"
    } ]
  }, {
    "metadata" : {
      "id" : "529BE6FCDA584E0A8C3D21579C3FCBE6"
    },
    "cell_type" : "markdown",
    "source" : "Why is the string shown twice? The first string is the output of `println` (\"print line\"). The second string is the value returned from `info` and assigned to the _immutable value_ (`val` keyword) named `infoString`. Note that if you didn't know what type of object was returned by `info`, Spark Notebook is showing you the type here, using the Scala syntax, `name: Type`.\n\nCheck out the right-hand side bar, too (using the _View > Toggle Sidebar_ menu item above). It shows the values we've defined so far and their types."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AAF85354A9654E22A53B00546A0CAA29"
    },
    "cell_type" : "code",
    "source" : "val errorString = error(\"Uh oh!\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "\n********************************************************************\n\n  ERROR: Uh oh!\n\n********************************************************************\n\nerrorString: String =\n\"\n********************************************************************\n\n  ERROR: Uh oh!\n\n********************************************************************\n\"\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 11,
      "time" : "Took: 1 second 220 milliseconds, at 2017-5-22 16:43"
    } ]
  }, {
    "metadata" : {
      "id" : "57CAB7BAD29C4C938AF2ADBE21B14170"
    },
    "cell_type" : "markdown",
    "source" : "We see the same multiline string twice for the same reason. If you have defined a variable previously in the notebook, you can see its value (actually, the result of calling `toString` on the value), by putting it in a cell by itself."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DA14FE765E6C4E5B848876DC5FB95DBC"
    },
    "cell_type" : "code",
    "source" : "infoString",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res19: String = All is well.\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "All is well."
      },
      "output_type" : "execute_result",
      "execution_count" : 12,
      "time" : "Took: 1 second 313 milliseconds, at 2017-5-22 16:43"
    } ]
  }, {
    "metadata" : {
      "id" : "5CBE146ACD0B4212AB27E476DB571F7E"
    },
    "cell_type" : "markdown",
    "source" : "Let's explain the details of method definitions. Here is `info` again, with the comments removed for clarity:\n```scala\ndef info(message: String): String = {\n    println(message)\n    message\n}\n```\n\nOkay, here are the gory details. Don't worry about remembering all of them now, but return to this list when you need a reminder. In order, a method definition has the following elements:\n\n* The `def` keyword.\n* The method's name (`info` in this case).\n* The argument list in parentheses. If there are two or more arguments, the `name:Type` items are comma-separated. If there are no arguments, the empty parentheses can be omitted. This is common for \"getter\"-like methods that simply return a field in an instance or something new, like `toString`. Note that the method arguments must include the `:Type`; Scala can't infer these types!\n* A colon followed by the type of the value returned by the method. This return type _can_ be inferred by Scala in most cases, so it's optional. However, I recommend always putting in the return type for readibility by users (and to avoid occasionally, subtle mistakes in type inference...).\n* An `=` (equals) sign that separates the method _signature_ from the _body_.\n* The body in braces `{ ... }`, although if the body consists of a single expression, the braces are optional.\n* The last expression in the body is used as the return value. The `return` keyword is optional and rarely used.\n* Semicolons (`;`) are inferred at the end of lines (in most cases) and rarely used."
  }, {
    "metadata" : {
      "id" : "CD393F164BDA4FA883DC4B8A5D00AE09"
    },
    "cell_type" : "markdown",
    "source" : "Look again at the argument list for `info`. It is `(message: String)`, where `message` is the argument name and its type is `String`. This convention for _type annotations_, `name: Type`, is also used for the return type, `error(...): String`. Again, type annotations are required by Scala for method arguments. They are optional in most cases for the return type. Scala can infer the types of most expressions and variable declarations, too.\n\nScala uses the same comment conventions as Java, `// ...` for a single line, and `/* ... */` for a comment block."
  }, {
    "metadata" : {
      "id" : "5755E06B4F334463BB852AD280667B3B"
    },
    "cell_type" : "markdown",
    "source" : "> **Note:** Expression vs. Statement\n\n> An _expression_ has a value, while a _statement_ does not. Hence, when we assign an expression to a variable, the value the expression returns is assigned to the variable. Most \"constructs\" in Scala are actually expressions, even `if` conditionals and `for` loops. In Java, those are statements."
  }, {
    "metadata" : {
      "id" : "2F67CEA53FCC4CD78E60C8471C77C032"
    },
    "cell_type" : "markdown",
    "source" : "Inside `error`, we used a combination _interpolated_ and _triple-quoted_ string with the syntax `s\"\"\"...\"\"\"`:\n* **Triple-quoted string:** `\"\"\"...\"\"\"`. Useful for embedding newlines, like we did inside `error`. (We'll see another benefit later.)\n* **String interpolation:** Invoked by putting `s` before the string, e.g., `s\"...\"` or `s\"\"\"...\"\"\"`. Lets us embed variable references and expressions, where the string conversion will be inserted automatically. For example: "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AFF78A0F505845BE88A3559B7958B24A"
    },
    "cell_type" : "code",
    "source" : "s\"\"\"Use braces for expressions: ${sc.version}.\nYou can omit the braces when just using a variable: $sc\nHowever, watch for ambiguities like ${sc}andextrastuff\"\"\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res21: String =\nUse braces for expressions: 2.1.0.\nYou can omit the braces when just using a variable: org.apache.spark.SparkContext@30141f88\nHowever, watch for ambiguities like org.apache.spark.SparkContext@30141f88andextrastuff\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "Use braces for expressions: 2.1.0.\nYou can omit the braces when just using a variable: org.apache.spark.SparkContext@30141f88\nHowever, watch for ambiguities like org.apache.spark.SparkContext@30141f88andextrastuff"
      },
      "output_type" : "execute_result",
      "execution_count" : 13,
      "time" : "Took: 1 second 371 milliseconds, at 2017-5-22 16:47"
    } ]
  }, {
    "metadata" : {
      "id" : "60FFAE1216ED45A480E3C68682B432DC"
    },
    "cell_type" : "markdown",
    "source" : "Another feature we used in our triple-quoted string is the ability to strip the leading whitespace off each line. The `stripMargin` method removes all whitespace before and including the `|`. This lets you indent those lines for proper code formatting, but not have that whitespace remain in the string. In the following example, the resulting string has blank lines at the beginning and end. Note what happens with whitespace before `line2`:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F85886E75E5A47C9861E3F65A8EB4267"
    },
    "cell_type" : "code",
    "source" : "s\"\"\"\n    |line 1\n    |  line 2\n    |\"\"\".stripMargin",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res23: String =\n\"\nline 1\n  line 2\n\"\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "\nline 1\n  line 2\n"
      },
      "output_type" : "execute_result",
      "execution_count" : 14,
      "time" : "Took: 1 second 243 milliseconds, at 2017-5-22 16:47"
    } ]
  }, {
    "metadata" : {
      "id" : "2D7671C27CAB4A678AB8F0DF595BF4F4"
    },
    "cell_type" : "markdown",
    "source" : "Like Java, character \"literals\" are specified single quotes, '/', while strings use double quotes, \"/\". Note the return types for the next two cells:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "7E816DA2EE374A858D48A7CA8DB629C1"
    },
    "cell_type" : "code",
    "source" : "'/'",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res25: Char = /\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/"
      },
      "output_type" : "execute_result",
      "execution_count" : 15,
      "time" : "Took: 1 second 50 milliseconds, at 2017-5-22 16:48"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DE8F13B8E30D4EDF86972B678C67FF04"
    },
    "cell_type" : "code",
    "source" : "\"/\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res27: String = /\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/"
      },
      "output_type" : "execute_result",
      "execution_count" : 16,
      "time" : "Took: 1 second 86 milliseconds, at 2017-5-22 16:48"
    } ]
  }, {
    "metadata" : {
      "id" : "9E292992E7CA45C6BCE2C1D1E2E38012"
    },
    "cell_type" : "markdown",
    "source" : "### Mutable Variables vs. Immutable Values\nWe've already seen how to declare an _immutable_ value, using the `val` keyword. Let's explore this a bit more:\n* `val immutableValue = ...`: Once initialized, we can't assign a _different_ value to `immutableValue`.\n* `var mutableVariable = ...`: We can assign new values to `mutableVariable` as often as we want.\n\nIt's _highly recommended_ that you only use `vals` unless you have a good reason for using mutability, which is a very common source of bugs!!"
  }, {
    "metadata" : {
      "id" : "5D406BE17F12428081AD53D22E347148"
    },
    "cell_type" : "markdown",
    "source" : "> **Note:** `val` is a _shallow_ declaration of immutability. Immutability doesn't affect the whole object graph. Specifically, we can't change what a `val` points to, but if the object itself is mutable, we can change it! For example, a Scala [Array](http://www.scala-lang.org/api/current/#scala.Array) is just a thin wrapper around Java arrays, which are mutable. Observe the following:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "85CA5B48E00B4FF99034AFC8EABAA172"
    },
    "cell_type" : "code",
    "source" : "val a = Array(1, 2, 3)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "a: Array[Int] = Array(1, 2, 3)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 17,
      "time" : "Took: 977 milliseconds, at 2017-5-22 16:49"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8FB20E6975634D42978906BAAF0D85C1"
    },
    "cell_type" : "code",
    "source" : "a = Array(4, 5, 6)  // not allowed",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:69: error: reassignment to val\n       a = Array(4, 5, 6)  // not allowed\n         ^\n"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1628998760-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "D6B0BBC900E34C469FD5DDF4F25EAAC0"
    },
    "cell_type" : "code",
    "source" : "a(1) = 20  // allowed!\na",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res31: Array[Int] = Array(1, 20, 3)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon169fbbe5911a2bebd5a97c2377a72020&quot;,&quot;dataInit&quot;:[],&quot;genId&quot;:&quot;1628998760&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <div>\n        <ul class=\"nav nav-tabs\" id=\"ul1628998760\"><li>\n              <a href=\"#tab1628998760-0\"><i class=\"fa fa-table\"/></a>\n            </li><li>\n              <a href=\"#tab1628998760-1\"><i class=\"fa fa-dot-circle-o\"/></a>\n            </li><li>\n              <a href=\"#tab1628998760-2\"><i class=\"fa fa-line-chart\"/></a>\n            </li><li>\n              <a href=\"#tab1628998760-3\"><i class=\"fa fa-bar-chart\"/></a>\n            </li><li>\n              <a href=\"#tab1628998760-4\"><i class=\"fa fa-cubes\"/></a>\n            </li></ul>\n\n        <div class=\"tab-content\" id=\"tab1628998760\"><div class=\"tab-pane\" id=\"tab1628998760-0\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon1fa42b477d878c843f9c9e231151712c&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:20},{&quot;_1&quot;:2,&quot;_2&quot;:3}],&quot;genId&quot;:&quot;2100784050&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"_1\",\"_2\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon6362272205c52643ca2c2bc9686dc5d5&quot;,&quot;initialValue&quot;:&quot;3&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon70404b6595f61363da2d282f92047a1d&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab1628998760-1\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon6a417b2e163bb6bc4ecc1ecdb3a7f1e4&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:20},{&quot;_1&quot;:2,&quot;_2&quot;:3}],&quot;genId&quot;:&quot;764501578&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/scatterChart'], \n      function(playground, _magicscatterChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicscatterChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonb816b6616dc2c5ed506c5435a72093fc&quot;,&quot;initialValue&quot;:&quot;3&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon01dce08929c6a1c3d455aea152943b8c&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab1628998760-2\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon6b6a29de4dcc87aa3c666d01862be7d8&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:20},{&quot;_1&quot;:2,&quot;_2&quot;:3}],&quot;genId&quot;:&quot;1177997903&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/lineChart'], \n      function(playground, _magiclineChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiclineChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anond45007fb424168dc72021fe421801cf3&quot;,&quot;initialValue&quot;:&quot;3&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anona77377b599abcc993139932d8204c853&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab1628998760-3\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon5d766fb01e21291d77cc2a30a65dc52d&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:20},{&quot;_1&quot;:2,&quot;_2&quot;:3}],&quot;genId&quot;:&quot;527140554&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/barChart'], \n      function(playground, _magicbarChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicbarChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon67ef655a29c033e937f0000a6aed7c1a&quot;,&quot;initialValue&quot;:&quot;3&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon58043eaba5bc3573e310f27c6779a647&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab1628998760-4\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon77bc0f2fd794178a02f7552e4a671e41&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:20},{&quot;_1&quot;:2,&quot;_2&quot;:3}],&quot;genId&quot;:&quot;386455739&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon23ed65071da164464a9e1d6cac59636c&quot;,&quot;initialValue&quot;:&quot;3&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon140a39c5da1012e7b0d0d8e50a28c33b&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div></div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 19,
      "time" : "Took: 1 second 643 milliseconds, at 2017-5-22 16:50"
    } ]
  }, {
    "metadata" : {
      "id" : "7CDC26D8E6E145579F07A2A577B264AA"
    },
    "cell_type" : "markdown",
    "source" : "### Download the Files\nNow let's define a method that works like the popularity \\*NIX [curl](http://linux.die.net/man/1/curl) utility. It's a bit long and you don't need to understand all the details, but we'll use it to download data we need for the notebook."
  }, {
    "metadata" : {
      "id" : "5956F2A6A32C4782844AE7E53A982575"
    },
    "cell_type" : "markdown",
    "source" : "Most of the types used here are from Java's library (JDK). Because Scala compiles to JVM byte code, you can use any Java library you want from Scala:\n* [java.net.URL](https://docs.oracle.com/javase/8/docs/api/java/net/URL.html): Handles URL formatting and connections.\n* [java.io.File](https://docs.oracle.com/javase/8/docs/api/java/io/File.html): Working with files and directories.\n* [java.io.BufferedInputStream](https://docs.oracle.com/javase/8/docs/api/java/io/BufferedInputStream.html): Buffered input from an underlying stream.\n* [java.io.BufferedOutputStream](https://docs.oracle.com/javase/8/docs/api/java/io/BufferedOutputStream.html): Buffered output to an underlying stream.\n* [java.io.FileOutputStream](https://docs.oracle.com/javase/8/docs/api/java/io/FileOutputStream.html): Output to a file, specifically.\n\nAs before, we'll use comments to explain a new Scala constructs as we go. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1CD9CBDB4CE540AE8C0CACD1106377A6"
    },
    "cell_type" : "code",
    "source" : "// Import this utility for working with URLs. Unlike Java the semicolon ';' is not required.\nimport java.net.URL   \n\n// Use {...} to provide a list of things to import, when you don't want to import everything \n// in a package and you don't want to write a separate line for each type.\nimport java.io.{File, BufferedInputStream, BufferedOutputStream, FileOutputStream}\n\n/**\n * Download a file at a URL and write it to a target directory.\n * @return the java.io.File for the downloaded file.\n */\ndef curl(sourceURLString: String, targetDirectoryString: String): File = {\n\n    // The path separator on your platform: \"/\" on Linux and MacOS, \"\\\" on Windows.\n    val pathSeparator = File.separator\n\n    // Use the name of the remote file as the file name in the target directory.\n    // We split on the URL path elements using the separator, which is ALWAYS \"/\"\n    // on all platforms for URLs. This gives us an array of path elements; the \n    // name will be the last one.\n    val sourceFileName = sourceURLString.split(\"/\").last  \n    val outFileName = targetDirectoryString + pathSeparator + sourceFileName\n\n    // Set up a connection and buffered input stream for the source file.\n    println(s\"Downloading $sourceURLString to $outFileName\")\n    val sourceURL = new URL(sourceURLString)\n    val connection = sourceURL.openConnection()\n    val in = new BufferedInputStream(connection.getInputStream())\n\n    // If here, the connection was successfully opened (i.e., no exceptions thrown).\n    // Now create the target directory (nothing happens if it already exists).\n    val targetDirectory = new File(targetDirectoryString)\n    targetDirectory.mkdirs()\n\n    // Setup the output file and a stream to write to it.\n    val outFile = new File(outFileName)\n    val out = new BufferedOutputStream(new FileOutputStream(outFile))\n    \n    // Create a buffer to hold the in-flight bytes.\n    val hundredK = 100*1024\n    val bytes = Array.fill[Byte](hundredK)(0)   // Create byte buffer, elements set to 0\n                                                // Array elements are _mutable_.\n    // Loop until we've read everything.\n    var loops = 0                               // A counter for progress feedback.\n    var count = in.read(bytes, 0, hundredK)     // Read up to \"hundredK\" bytes at a time.\n    while (count != -1) {                       // Haven't hit the end of input yet?\n        if (loops % 10 == 0) print(\".\")         // Print occasional feedback.\n        loops += 1                              // increment the counter.\n        out.write(bytes, 0, count)              // Write to the new file.\n        count = in.read(bytes, 0, hundredK)     // Read the next chunk and loop...\n    }\n    println(\"\\nFinished!\")\n    in.close()                                  // Clean up! Close file & stream handles\n    out.flush()\n    out.close()\n    outFile                                     // Returned file (if we got this far)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import java.net.URL\nimport java.io.{File, BufferedInputStream, BufferedOutputStream, FileOutputStream}\ncurl: (sourceURLString: String, targetDirectoryString: String)java.io.File\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 20,
      "time" : "Took: 1 second 160 milliseconds, at 2017-5-22 16:52"
    } ]
  }, {
    "metadata" : {
      "id" : "9CDE2B4DECD04454B032AB2965658AC3"
    },
    "cell_type" : "markdown",
    "source" : "Okay, before we actually use `curl`, let's create the target directory. (This is also done in `curl`, but we're using the success or failure for other purposes here.) "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "2694A3E4B4FF44F683842A9AB7742C89"
    },
    "cell_type" : "code",
    "source" : "// The target directory, which we'll now create, if necessary.\nval shakespeare = new File(\"data/shakespeare\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "shakespeare: java.io.File = data/shakespeare\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 21,
      "time" : "Took: 1 second, at 2017-5-22 16:53"
    } ]
  }, {
    "metadata" : {
      "id" : "2C4EA916B7974EAE8863CDB8D78251B7"
    },
    "cell_type" : "markdown",
    "source" : "Scala's `if` construct is actually an expression (in Java they are _statements_). The `if` expression will return `true` or `false` and assign it to `success`, which we'll use in a moment."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CFADB7E042BC4F59837EA8CBCE16CDB2"
    },
    "cell_type" : "code",
    "source" : "val success = if (shakespeare.exists == false) {   // doesn't exist already? In Java, I would need parentheses: .exists()\n    if (shakespeare.mkdirs() == false) {           // did the attempt fail??\n        error(s\"Failed to create directory path: $shakespeare\")  // ignore returned string\n        false\n    } else {                                       // successful\n        info(s\"Created $shakespeare\")\n        true\n    }\n} else {\n    info(s\"$shakespeare already exists\")\n    true\n}\nprintln(\"success = \" + success)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Created data/shakespeare\nsuccess = true\nsuccess: Boolean = true\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 22,
      "time" : "Took: 1 second 358 milliseconds, at 2017-5-22 16:54"
    } ]
  }, {
    "metadata" : {
      "id" : "015D2C9460EC4C619EEDE8AF53019887"
    },
    "cell_type" : "markdown",
    "source" : "If we successfully created the output directory (or it already existed), let's download a handful of files, each with one play of Shakespeare, from [http://www.cs.usyd.edu.au/~matty/Shakespeare/](http://www.cs.usyd.edu.au/~matty/Shakespeare/)."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "FD299F503F6E45128FB1243B1E4C5025"
    },
    "cell_type" : "code",
    "source" : "val pathSeparator = File.separator\nval targetDirName = shakespeare.toString\nval urlRoot = \"http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/\"\nval plays = Seq(\n    \"tamingoftheshrew\", \"comedyoferrors\", \"loveslabourslost\", \"midsummersnightsdream\",\n    \"merrywivesofwindsor\", \"muchadoaboutnothing\", \"asyoulikeit\", \"twelfthnight\")\n\nif (success) {\n    println(s\"Downloading plays from $urlRoot.\")\n  val successes = for {\n        play <- plays\n        playFileName = targetDirName + pathSeparator + play\n        playFile = new File(playFileName)\n        if (playFile.exists == false) \n        file = curl(urlRoot + play, targetDirName)\n    } yield {\n        info(s\"Downloaded $play and wrote $file\")\n        s\"$playFileName:\\tSuccess!\"\n    }\n  \n    println(\"Finished!\")\n  \n    successes.foreach(println)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Downloading plays from http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/.\nDownloading http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/tamingoftheshrew to data/shakespeare/tamingoftheshrew\n.........\nFinished!\nDownloading http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/comedyoferrors to data/shakespeare/comedyoferrors\n.......\nFinished!\nDownloading http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/loveslabourslost to data/shakespeare/loveslabourslost\n.........\nFinished!\nDownloading http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/midsummersnightsdream to data/shakespeare/midsummersnightsdream\n.......\nFinished!\nDownloading http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/merrywivesofwindsor to data/shakespeare/merrywivesofwindsor\n..........\nFinished!\nDownloading http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/muchadoaboutnothing to data/shakespeare/muchadoaboutnothing\n.........\nFinished!\nDownloading http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/asyoulikeit to data/shakespeare/asyoulikeit\n..........\nFinished!\nDownloading http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/twelfthnight to data/shakespeare/twelfthnight\n........\nFinished!\nDownloaded tamingoftheshrew and wrote data/shakespeare/tamingoftheshrew\nDownloaded comedyoferrors and wrote data/shakespeare/comedyoferrors\nDownloaded loveslabourslost and wrote data/shakespeare/loveslabourslost\nDownloaded midsummersnightsdream and wrote data/shakespeare/midsummersnightsdream\nDownloaded merrywivesofwindsor and wrote data/shakespeare/merrywivesofwindsor\nDownloaded muchadoaboutnothing and wrote data/shakespeare/muchadoaboutnothing\nDownloaded asyoulikeit and wrote data/shakespeare/asyoulikeit\nDownloaded twelfthnight and wrote data/shakespeare/twelfthnight\nFinished!\ndata/shakespeare/tamingoftheshrew:\tSuccess!\ndata/shakespeare/comedyoferrors:\tSuccess!\ndata/shakespeare/loveslabourslost:\tSuccess!\ndata/shakespeare/midsummersnightsdream:\tSuccess!\ndata/shakespeare/merrywivesofwindsor:\tSuccess!\ndata/shakespeare/muchadoaboutnothing:\tSuccess!\ndata/shakespeare/asyoulikeit:\tSuccess!\ndata/shakespeare/twelfthnight:\tSuccess!\npathSeparator: String = /\ntargetDirName: String = data/shakespeare\nurlRoot: String = http://www.cs.usyd.edu.au/~matty/Shakespeare/texts/comedies/\nplays: Seq[String] = List(tamingoftheshrew, comedyoferrors, loveslabourslost, midsummersnightsdream, merrywivesofwindsor, muchadoaboutnothing, asyoulikeit, twelfthnight)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 23,
      "time" : "Took: 10 seconds 955 milliseconds, at 2017-5-22 16:56"
    } ]
  }, {
    "metadata" : {
      "id" : "8D9C6981C7CE4E79BAC96837D15A4EC8"
    },
    "cell_type" : "markdown",
    "source" : "I'm using a so-called `for` _comprehension_. They are _expressions_, not _statements_ like Java's `for` loops. They have the form:\n\n```\nfor {\n  play <- plays\n  ...\n} yield { block_of_final_expressions }\n```\n\nWe iterate through a collection, `plays`, and assign each one to the `play` variable (actually an immutable value for each pass through the loop). \n\nAfter assigning to `play`, subsequent steps in the `for` comprehension use it. First, a [java.io.File](https://docs.oracle.com/javase/8/docs/api/java/io/File.html) instance, `playFile`, is created. Then, `playFile` is used to evaluate a conditional - does the file already exist (i.e., have we already downloaded this file)?\n\nIf the file already exists, the conditional returns `false`, which short-circuits the loop and goes to the next `play` in the list. If the file doesn't exit, the final expression uses `curl` to download it.\n\nThe `yield` keyword tells Scala that I want to construct a new collection, using the expression that follows to construct each element, an _interpolated_ string."
  }, {
    "metadata" : {
      "id" : "E01A4A3D5726474381C300B424B54568"
    },
    "cell_type" : "markdown",
    "source" : "## Functions\n\nNotice that `collection.foreach(println)` in the previous code cell. This is our first example of a _function_ being used. Normally functions are passed as arguments to _methods_. This is an incredibly powerful technique; we write a method that does some \"general\" work, then pass it a function to control the details in a particular context. \n\nIn this case, `collection.foreach(...)` iterates through `collection` (so we don't have to write this boilerplate ourselves), then we pass a function to it that will be applied to each element in the collection. The `foreach` returns nothing (a special type `Unit` actually, which is like `void` in Java). So, we use it here to simply print the values, by passing `println` as the function."
  }, {
    "metadata" : {
      "id" : "B20562D4F213455A8D71DD3543FFEDCA"
    },
    "cell_type" : "markdown",
    "source" : "> **Note:** Actually `println` is itself a _method_, but when we use it in a function context like this, Scala treats it like a _function_. This is why in many cases we can ignore the distinction between _methods_ and _functions_."
  }, {
    "metadata" : {
      "id" : "3294471AEA464DED856AAD4DEC34DDB3"
    },
    "cell_type" : "markdown",
    "source" : "Let's look at more examples of functions. We'll use `plays` instead of `successes`, because the latter will be empty if you run that code cell more than once, since the files will already be downloaded!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "31C5F9D00B614B048C437556C1C0C047"
    },
    "cell_type" : "code",
    "source" : "println(\"Pass println as the function to use for each element:\")\nplays.foreach(println)\n\nprintln(\"\\nUsing an anonymous function that calls println: `str => println(str)`\")\nprintln(\"(Note that the type of the argument `str` is inferred to be String.)\")\nplays.foreach(str => println(str))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Pass println as the function to use for each element:\ntamingoftheshrew\ncomedyoferrors\nloveslabourslost\nmidsummersnightsdream\nmerrywivesofwindsor\nmuchadoaboutnothing\nasyoulikeit\ntwelfthnight\n\nUsing an anonymous function that calls println: `str => println(str)`\n(Note that the type of the argument `str` is inferred to be String.)\ntamingoftheshrew\ncomedyoferrors\nloveslabourslost\nmidsummersnightsdream\nmerrywivesofwindsor\nmuchadoaboutnothing\nasyoulikeit\ntwelfthnight\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 24,
      "time" : "Took: 2 seconds 61 milliseconds, at 2017-5-22 17:5"
    } ]
  }, {
    "metadata" : {
      "id" : "0A197922B6FF4662A8B9C978C296F8F0"
    },
    "cell_type" : "markdown",
    "source" : "So _anonymous_ functions (i.e., those that don't have a name) are written `argument_list => body`.\n\n> **Note:** Recall that _method_ arguments have to be declared with types. That's usually _not_ required for _function_ arguments, as shown here.\n"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9B3811BAC9194A7AA30DDB65678F075A"
    },
    "cell_type" : "code",
    "source" : "println(\"\\nAdding the argument type explicitly. Note that the parentheses are required.\")\nplays.foreach((str: String) => println(str))\n\nprintln(\"\\nFor longer functions, you can use {...} instead of (...).\")\nprintln(\"Why? Because it gives you the familiar multiline block syntax with {...}\")\nplays.foreach {\n  (str: String) => println(str)\n}\n\nprintln(\"\\nWhy do we need to name this argument? Scala lets us use _ as a placeholder.\")\nplays.foreach(println(_))\n\nprintln(\"\\nThe _ placeholder can be used *once* for each argument in the list.\")\nprintln(\"As an assume, use `reduceLeft` to sum some integers.\")\nval integers = 0 to 10   // Return a \"range\" from 0 to 10, inclusive\nintegers.reduceLeft((i,j) => i+j)\nintegers.reduceLeft(_+_)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "\nAdding the argument type explicitly. Note that the parentheses are required.\ntamingoftheshrew\ncomedyoferrors\nloveslabourslost\nmidsummersnightsdream\nmerrywivesofwindsor\nmuchadoaboutnothing\nasyoulikeit\ntwelfthnight\n\nFor longer functions, you can use {...} instead of (...).\nWhy? Because it gives you the familiar multiline block syntax with {...}\ntamingoftheshrew\ncomedyoferrors\nloveslabourslost\nmidsummersnightsdream\nmerrywivesofwindsor\nmuchadoaboutnothing\nasyoulikeit\ntwelfthnight\n\nWhy do we need to name this argument? Scala lets us use _ as a placeholder.\ntamingoftheshrew\ncomedyoferrors\nloveslabourslost\nmidsummersnightsdream\nmerrywivesofwindsor\nmuchadoaboutnothing\nasyoulikeit\ntwelfthnight\n\nThe _ placeholder can be used *once* for each argument in the list.\nAs an assume, use `reduceLeft` to sum some integers.\nintegers: scala.collection.immutable.Range.Inclusive = Range(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\nres41: Int = 55\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "55"
      },
      "output_type" : "execute_result",
      "execution_count" : 25,
      "time" : "Took: 1 second 526 milliseconds, at 2017-5-22 17:7"
    } ]
  }, {
    "metadata" : {
      "id" : "6E8F041BF67F43DD9C79FE29021BDC7F"
    },
    "cell_type" : "markdown",
    "source" : "# Our First Spark Program\nWhew! We've learned a lot of Scala already while doing typical data science chores (i.e., fetching data). \n\nNow let's implement a real algorithm using Spark, _Inverted Index_."
  }, {
    "metadata" : {
      "id" : "EA38A54959954056A26AFB2136540AD4"
    },
    "cell_type" : "markdown",
    "source" : "## Inverted Index - When You're Tired of Counting Words...\n\nYou'll want to use _Inverted Index_ when you create your next \"Google killer\". It takes in a corpus of documents (e.g., web pages), tokenizes the words, and outputs for each word a list of the documents that contain it, along with the corresponding counts. \n\nThis is a slightly more interesting algorithm than _Word Count_, the classic \"hello world\" program everyone implements when they learn Spark.\n\nThe term _inverted_ here means we start with the words as part of the input _values_, while the _keys_ are the document identifiers, and we'll switch (\"invert\") to using the words as keys and the document identifiers as values."
  }, {
    "metadata" : {
      "id" : "D7F7A6A97DE14FA18131C707E4599D8A"
    },
    "cell_type" : "markdown",
    "source" : "Here's our first version, all at once. This is _one, long expression_. Note the periods `.` at the end of the subexpressions."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "279CFDA0E697461A8109AF880AF3FF10"
    },
    "cell_type" : "code",
    "source" : "val iiFirstPass1 = sc.wholeTextFiles(shakespeare.toString).\n    flatMap { location_contents_tuple2 => \n        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n        words.map(word => ((word, fileName), 1))\n    }.\n    reduceByKey((count1, count2) => count1 + count2).\n    map { word_file_count_tup3 => \n        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n    }.\n    groupByKey.\n    sortByKey(ascending = true).\n    mapValues { iterable => \n        val vect = iterable.toVector.sortBy { file_count_tup2 => \n            (-file_count_tup2._2, file_count_tup2._1)\n        }\n        vect.mkString(\",\")\n    }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iiFirstPass1: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[9] at mapValues at <console>:85\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 26,
      "time" : "Took: 5 seconds 275 milliseconds, at 2017-5-22 17:12"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6EB201ECBECA4B148CC0D6E7D308C46D"
    },
    "cell_type" : "code",
    "source" : "iiFirstPass1.take(50).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(,(asyoulikeit,1),(comedyoferrors,1),(loveslabourslost,1),(merrywivesofwindsor,1),(midsummersnightsdream,1),(muchadoaboutnothing,1),(tamingoftheshrew,1),(twelfthnight,1))\n(A,(loveslabourslost,78),(tamingoftheshrew,59),(twelfthnight,47),(comedyoferrors,42),(midsummersnightsdream,39),(merrywivesofwindsor,38),(asyoulikeit,34),(muchadoaboutnothing,31))\n(ABOUT,(muchadoaboutnothing,18))\n(ACT,(merrywivesofwindsor,23),(asyoulikeit,22),(twelfthnight,18),(muchadoaboutnothing,17),(tamingoftheshrew,12),(comedyoferrors,11),(loveslabourslost,9),(midsummersnightsdream,9))\n(ADAM,(asyoulikeit,16))\n(ADO,(muchadoaboutnothing,18))\n(ADRIANA,(comedyoferrors,85))\n(ADRIANO,(loveslabourslost,111))\n(AEGEON,(comedyoferrors,20))\n(AEMELIA,(comedyoferrors,16))\n(AEMILIA,(comedyoferrors,3))\n(AEacides,(tamingoftheshrew,1))\n(AEgeon,(comedyoferrors,7))\n(AEgle,(midsummersnightsdream,1))\n(AEmilia,(comedyoferrors,4))\n(AEsculapius,(merrywivesofwindsor,1))\n(AGUECHEEK,(twelfthnight,2))\n(ALL,(midsummersnightsdream,2),(tamingoftheshrew,2))\n(AMIENS,(asyoulikeit,16))\n(ANDREW,(twelfthnight,104))\n(ANGELO,(comedyoferrors,36))\n(ANN,(merrywivesofwindsor,1))\n(ANNE,(merrywivesofwindsor,27))\n(ANTIPHOLUS,(comedyoferrors,195))\n(ANTONIO,(muchadoaboutnothing,32),(twelfthnight,32))\n(ARMADO,(loveslabourslost,111))\n(AS,(asyoulikeit,24))\n(AUDREY,(asyoulikeit,18))\n(Abate,(loveslabourslost,1),(midsummersnightsdream,1))\n(Abbess,(comedyoferrors,2))\n(Abetting,(comedyoferrors,1))\n(Abhor,(asyoulikeit,1))\n(Abide,(midsummersnightsdream,1))\n(Ability,(muchadoaboutnothing,1))\n(About,(loveslabourslost,2),(merrywivesofwindsor,2),(midsummersnightsdream,1),(muchadoaboutnothing,1),(tamingoftheshrew,1),(twelfthnight,1))\n(Above,(loveslabourslost,3),(twelfthnight,2),(merrywivesofwindsor,1),(tamingoftheshrew,1))\n(Abraham,(merrywivesofwindsor,2))\n(Abruptly,(asyoulikeit,1))\n(Academe,(loveslabourslost,1))\n(Accept,(tamingoftheshrew,1))\n(Accompany,(midsummersnightsdream,1))\n(According,(asyoulikeit,2),(comedyoferrors,1),(tamingoftheshrew,1),(twelfthnight,1))\n(Accost,(twelfthnight,3))\n(Accusativo,(merrywivesofwindsor,1))\n(Acheron,(midsummersnightsdream,1))\n(Achieve,(tamingoftheshrew,1))\n(Achilles,(loveslabourslost,1))\n(Actaeon,(merrywivesofwindsor,2))\n(Action,(loveslabourslost,1))\n(Adam,(asyoulikeit,7),(comedyoferrors,4),(muchadoaboutnothing,3),(loveslabourslost,2),(tamingoftheshrew,1))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 27,
      "time" : "Took: 1 second 935 milliseconds, at 2017-5-22 17:12"
    } ]
  }, {
    "metadata" : {
      "id" : "5C58024E869A433188F2002554FD0DD8"
    },
    "cell_type" : "markdown",
    "source" : "Now let's break it down into steps, assigning each step to a variable. This extra verbosity let's us see what Scala infers for the type returned by each expression, helping us learn. \n\nThis is one of the nice features of Scala. We don't have to put in the type information ourselves, most of the time, like we would have to do for Java code. Instead, we let the compiler give us feedback about what we just created. This is especially useful when you're learning a new API, like Spark's."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5C14F35215B645AF8E1114F13E06BE84"
    },
    "cell_type" : "code",
    "source" : "val fileContents = sc.wholeTextFiles(shakespeare.toString)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "fileContents: org.apache.spark.rdd.RDD[(String, String)] = data/shakespeare MapPartitionsRDD[11] at wholeTextFiles at <console>:71\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 28,
      "time" : "Took: 2 seconds 23 milliseconds, at 2017-5-22 17:18"
    } ]
  }, {
    "metadata" : {
      "id" : "3DE80FDA9C8947938A39BDB23B6A7E59"
    },
    "cell_type" : "markdown",
    "source" : "The output is telling us that `fileContents` has the type [RDD[(String,String)]](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), but `RDD` is a base class and the actual instance is a `MapPartitionsRDD`, which is a \"private\" implementation subclass of `RDD`. \n\nA name followed by square brackets, `[...]`, means that `RDD[...]` requires one or more type parameters in the brackets. In this case, a single type parameter, which represents the type of the records held by the `RDD`. \n\nThe single type parameter is given by `(String,String)`, which is a convenient shorthand for [Tuple2[String,String]](http://www.scala-lang.org/api/current/index.html#scala.Tuple2). That is, we have two-element _tuples_ as records, where the first element is a `String` for a file's fully-qualified path and the second element is a `String` for the contents of that file. This is what `SparkContext.wholeTextFiles` returns for us. We'll use the path to remember where we found words, while the contents contains the words themselves (of course).\n\nTo recap, the following two types are equivalent:\n* `RDD[(String,String)]` - Note parentheses nested in brackets, `[(...)]`.\n* `RDD[Tuple2[String,String]]` - Note nested brackets `[...[...]]`, not `[(...)]`.\n\n> **NOTE:** The number in brackets `[N]` in `MapPartitionsRDD[N]` is actually an internal identifier used by Spark. In the output for `fileContents`, we're actually calling `toString` on the instance. It's confusing that `MapPartitionsRDD.toString` uses `[...]`, which looks like a type signature."
  }, {
    "metadata" : {
      "id" : "E7013006BEBB49818F71394C6006B8E6"
    },
    "cell_type" : "markdown",
    "source" : "We'll see shortly that you can also write _instances_ of [Tuple2[T1,T2]](http://www.scala-lang.org/api/current/index.html#scala.Tuple2) with the same syntax, e.g., `(\"foo\", 101)`, for a `(String,Int)` tuple, and similarly for _higher-arity_ tuples (up to 22 elements...), e.g., `(\"foo\", 101, 3.14159, (\"bar\", 202L))`. Run the next cell to see the type signature for this last tuple."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "348ECFFC072E498A90A069E8FBD896C6"
    },
    "cell_type" : "code",
    "source" : "val footuple = (\"foo\", 101L, 3.14159F, (\"bar\", 202L))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "footuple: (String, Long, Float, (String, Long)) = (foo,101,3.14159,(bar,202))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 29,
      "time" : "Took: 1 second 110 milliseconds, at 2017-5-22 17:19"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6F0388F3C9524011BB44099F00976C05"
    },
    "cell_type" : "code",
    "source" : "val (name, age, pi, (otherName, otherAge)) = footuple",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "name: String = foo\nage: Long = 101\npi: Float = 3.14159\notherName: String = bar\notherAge: Long = 202\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 30,
      "time" : "Took: 1 second 98 milliseconds, at 2017-5-22 17:19"
    } ]
  }, {
    "metadata" : {
      "id" : "D3DDA1765CFF488C9C09518D2B7A6709"
    },
    "cell_type" : "markdown",
    "source" : "Do you understand it? Do you see that it's a four-element tuple and not a five-element tuple? This is because the `(\"bar\", 202L)` is a nested tuple. It's the fourth element of the outer tuple."
  }, {
    "metadata" : {
      "id" : "560F2DC1EAAD46F28CDDBB2F5BE877ED"
    },
    "cell_type" : "markdown",
    "source" : "**Exercise:** Using the next cell, try creating some more tuples with elements of different types. Trying writing assignment statements that decompose the tuple, as in the previous code cell."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E9EA7CFFCEE24419B028DE88C7AE70A3"
    },
    "cell_type" : "code",
    "source" : "(1,2)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res49: (Int, Int) = (1,2)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "(1,2)"
      },
      "output_type" : "execute_result",
      "execution_count" : 31,
      "time" : "Took: 1 second 4 milliseconds, at 2017-5-22 17:19"
    } ]
  }, {
    "metadata" : {
      "id" : "DC32974A3B8D40F08D6F1717F6C5B6D2"
    },
    "cell_type" : "markdown",
    "source" : "How many `fileContents` records do we have? Not many. It should be the same number as the number of files we downloaded above."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6A25871A32B14E89908FA23FD5358182"
    },
    "cell_type" : "code",
    "source" : "fileContents.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res51: Long = 8\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "8"
      },
      "output_type" : "execute_result",
      "execution_count" : 32,
      "time" : "Took: 1 second 819 milliseconds, at 2017-5-22 17:19"
    } ]
  }, {
    "metadata" : {
      "id" : "B63454514AF841A7824D0A78CCE324C6"
    },
    "cell_type" : "markdown",
    "source" : "> **NOTE:** We called the `RDD.count` method, whereas most Scala collections have a `size` method."
  }, {
    "metadata" : {
      "id" : "BF118FEFB3CA44DB8065BAEC0F0765DD"
    },
    "cell_type" : "markdown",
    "source" : "Let's look at the data, but recall that each record contains the entire contents of one play, so let's not just print the whole record, but the first 200 characters of the play."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "36EF21C8E62149838CF404DC4BC1E803"
    },
    "cell_type" : "code",
    "source" : "fileContents.take(1).foreach(name_play => println(s\"${name_play._1}:\\n${name_play._2.take(200)}\"))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "file:/Users/deanwampler/projects/spark/spark-notebook/spark-notebook-0.7.0-scala-2.11.8-spark-2.1.0-hadoop-2.7.2-with-hive/data/shakespeare/asyoulikeit:\n\tAS YOU LIKE IT\n\n\n\tDRAMATIS PERSONAE\n\n\nDUKE SENIOR\tliving in banishment.\n\nDUKE FREDERICK\this brother, an usurper of his dominions.\n\n\nAMIENS\t|\n\t|  lords attending on the banished duke.\nJAQUES\t|\n\n\nLE BE\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 33,
      "time" : "Took: 1 second 573 milliseconds, at 2017-5-22 17:19"
    } ]
  }, {
    "metadata" : {
      "id" : "6A7FE239825A4DAE86368FAF6E09A142"
    },
    "cell_type" : "markdown",
    "source" : "Now for our next step in the calculation. First, \"tokenize\" the contents into words by splitting on non-alphanumeric characters, meaning all runs of whitespace (including the newlines), punctuation, etc.\n\nNext, the fully-qualified path is verbose and the same prefix is repeated for all the files, so let's extract just the last element of it, the unique file name.\n\nThen form new tuples with the words and file names.\n\n> **Note:** This \"tokenization\" approach is very crude. It improperly handles contractions, like `it's` and hyphenated words like `world-changing`. When you kill Google, be sure to use a real _natural language processing_ (NLP) tokenization technique."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E072D355D0FF43B58F08848DEAC6109C"
    },
    "cell_type" : "code",
    "source" : "val wordFileNameOnes = fileContents.flatMap { location_contents_tuple2 => // i.e., (file_path, \"all the words in the file\")\n    val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")              // mytuple._2 => give me the 2nd element\n    val fileName = location_contents_tuple2._1.split(pathSeparator).last  // mytuple._1 => give me the 1st element\n    words.map(word => ((word, fileName), 1))        // create a new tuple to return. Note how we structured it!\n}\nwordFileNameOnes",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "wordFileNameOnes: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[12] at flatMap at <console>:75\nres55: org.apache.spark.rdd.RDD[((String, String), Int)] = MapPartitionsRDD[12] at flatMap at <console>:75\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[12] at flatMap at &lt;console&gt;:75"
      },
      "output_type" : "execute_result",
      "execution_count" : 34,
      "time" : "Took: 961 milliseconds, at 2017-5-22 17:20"
    } ]
  }, {
    "metadata" : {
      "id" : "2E98B66DEE2140378F9FA7377ADF1A9A"
    },
    "cell_type" : "markdown",
    "source" : "I find this hard to read and shortly I'll show you a much more elegant, alternative syntax.\n\nLet's understand the difference between `map` and `flatMap`. If I called `fileContents.map`, it would return exactly _one_ new record for each record in _fileContents_. What I actually want instead are new records for each word-fileName combination, a significantly larger number (but the data in each record will be much smaller). \n\nUsing `fileContents.flatMap` gives me what I want. Instead of returning one output record for each input record, a `flatMap` returns a _collection_ of new records, zero or more, for _each_ input record. These collections are then _flattened_ into one big collection, another `RDD` in this case."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B9A6AE4D94A443BDA467B4D9EBBE43FC"
    },
    "cell_type" : "code",
    "source" : "wordFileNameOnes.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res57: Long = 173336\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "173336"
      },
      "output_type" : "execute_result",
      "execution_count" : 35,
      "time" : "Took: 1 second 452 milliseconds, at 2017-5-22 17:20"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DB9A436FE9DD48C18E0C1B37B6E73999"
    },
    "cell_type" : "code",
    "source" : "wordFileNameOnes.take(10).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "((,asyoulikeit),1)\n((AS,asyoulikeit),1)\n((YOU,asyoulikeit),1)\n((LIKE,asyoulikeit),1)\n((IT,asyoulikeit),1)\n((DRAMATIS,asyoulikeit),1)\n((PERSONAE,asyoulikeit),1)\n((DUKE,asyoulikeit),1)\n((SENIOR,asyoulikeit),1)\n((living,asyoulikeit),1)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 36,
      "time" : "Took: 1 second 284 milliseconds, at 2017-5-22 17:20"
    } ]
  }, {
    "metadata" : {
      "id" : "FB33105E2CF641E7AE3BB637A5A0AA82"
    },
    "cell_type" : "markdown",
    "source" : "What should `flatMap` actually do with each record? I pass a _function_ to define what to do. I'm using an unnamed or _anonymous_ function. The syntax is `argument_list => body`:\n\n```scala\nlocation_contents_tuple2 => \n    val words = ...\n    ...\n}\n```\n\nI have a single argument, the record, which I named `location_contents_tuple2`, a verbose way to say that it's a two-element tuple with an input file's location and contents. I don't require a type parameter after `location_contents_tuple2`, because it's inferred by Scala. The `=>` \"arrow\" separates the argument list from the body, which appears on the next few lines.\n\nWhen a function takes more than one argument or you add explicit type _annotations_ (e.g., `: (String,Int,Double)`), then you need parentheses. Here are three examples:\n\n```scala\n(some_tuple3: (String,Int,Double)) => ...\n(arg1, arg2, arg3) => ...\n(arg1: String, arg2: Int, arg3: Double) => ...\n```\nWe're letting Scala infer the argument type in our case, `(String,String)`."
  }, {
    "metadata" : {
      "id" : "1053B25F426A40558F2F51C474EC8998"
    },
    "cell_type" : "markdown",
    "source" : "Wait, I said we're passing a function as an argument to `flatMap`. If so, why am I using braces `{...}` around this function argument instead of parentheses `(...)` like you would normally expect when passing arguments to a method like `flatMap`? \n\nIt's because Scala lets us substitute braces instead of parentheses so we have the familiar block-like syntax `{...}` we know and love for `if` and `for` expressions. I could use either braces or parentheses here. The convention in the Scala community is to use braces for a multi-line anonymous function and to use parentheses for a single expression when it fits on the same line."
  }, {
    "metadata" : {
      "id" : "E6C1137A823349E082F92E38C1F3FACF"
    },
    "cell_type" : "markdown",
    "source" : "Now, for each `location_contents_tuple2`, I access the _first_ element using the `_1` method and the _second_ element using `_2`.\n\nThe file `contents` is in the second element. I split it by calling Java's `String.split` method, which takes a _regular expression_ string. Here I specify a regular expression for one or more, non-alphanumeric characters. `String.split` returns an `Array[String]` of the words. \n\n```scala\nval words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n```\n\nFor the first tuple element, I extract the file name at the end of the location path. This isn't necessary, but it makes the output more readable if I remove the long, common prefix from the path. \n\n```scala\nval fileName = location_contents_tuple2._1.split(pathSeparator).last\n```"
  }, {
    "metadata" : {
      "id" : "CEF8F60B111D4EE8B4F08C0DAF4E3701"
    },
    "cell_type" : "markdown",
    "source" : "Finally, still inside the anonymous function passed to `flatMap`, I use Scala's `Array.map` (_not_ `RDD.map`) to transform each `word` into a tuple of the form `((word, fileName), 1)`.\n\n```scala\nwords.map(word => ((word, fileName), 1))\n```\n\nWhy did I embed a tuple of `(word, fileName)` inside the \"outer\" tuple with a `1` as the second element? Why not just write a three-element tuple, `(word, fileName, 1)`? It's because I'll use the `(word, fileName)` as a _key_ in the next step, where I'll find all unique word-fileName combinations (using the equivalent of a `group by` statement). So, using the nested `(word, fileName)` as my _key_ is most convenient. The `1` _value_ is a \"seed\" count, which I'll use to count the occurrences of the unique `(word, fileName)` pairs."
  }, {
    "metadata" : {
      "id" : "CB365E8B50C44DC88A0A4FB973D1AF0D"
    },
    "cell_type" : "markdown",
    "source" : "> **Notes:**\n> * For historical reasons, tuple indices start at 1, not 0. Arrays and other Scala collections index from 0 (like in Java).\n> * Recall that _method_ arguments have to be declared with types, but that's usually _not_ required for _function_ arguments.\n> * Another benefit of triple-quoted strings that makes them nice for regular expressions is that you don't have to escape regular expression metacharacters, like `\\W`. If I used a single-quoted string, I would have to write it as `\"\\\\W+\"`. Your choice..."
  }, {
    "metadata" : {
      "id" : "08AA193D2F2043FA831DD736760C6CB8"
    },
    "cell_type" : "markdown",
    "source" : "Let's count the number of records we have and look at a few of the lines. We'll use the `RDD.take` method to grab the first 10 lines, then loop over them and print them."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "41FF9807CE0B4FB59C59DEEA48A7E8A2"
    },
    "cell_type" : "code",
    "source" : "wordFileNameOnes.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res61: Long = 173336\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "173336"
      },
      "output_type" : "execute_result",
      "execution_count" : 37,
      "time" : "Took: 1 second 667 milliseconds, at 2017-5-22 17:24"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "AA2C8482A4874A2584B6DE64D387FE7D"
    },
    "cell_type" : "code",
    "source" : "wordFileNameOnes.take(10).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "((,asyoulikeit),1)\n((AS,asyoulikeit),1)\n((YOU,asyoulikeit),1)\n((LIKE,asyoulikeit),1)\n((IT,asyoulikeit),1)\n((DRAMATIS,asyoulikeit),1)\n((PERSONAE,asyoulikeit),1)\n((DUKE,asyoulikeit),1)\n((SENIOR,asyoulikeit),1)\n((living,asyoulikeit),1)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 38,
      "time" : "Took: 1 second 643 milliseconds, at 2017-5-22 17:24"
    } ]
  }, {
    "metadata" : {
      "id" : "8BA9250ABA92441FA406F42C09C867AD"
    },
    "cell_type" : "markdown",
    "source" : "We asked for results, so we forced Spark to run a job to compute results. Spark pipelines, like `iiFirstPass1` are _lazy_; nothing is computed until we ask for results. \n\nWhen you're learning, it's useful to print some data to better understand what's happening. Just be aware of the extra overhead of running lots of Spark jobs.\n\nThe first record shown has \"\" (blank) as the word:\n\n```\n((,asyoulikeit),1)\n```\n\nAlso, some words have all capital letters:\n\n```\n((DRAMATIS,asyoulikeit),1)\n```\n\n(You can see where these capitalized words occur if you look in the original files.) Later on, We'll filter out the blank-word records and use lower case for all words."
  }, {
    "metadata" : {
      "id" : "82C274F9C74448B7817C504E07F920E3"
    },
    "cell_type" : "markdown",
    "source" : "Now, let's join all the unique `(word,fileName)` pairs together. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8DD3365A4AA2402A9D2556F995961C00"
    },
    "cell_type" : "code",
    "source" : "val uniques = wordFileNameOnes.reduceByKey((count1, count2) => count1 + count2)\nuniques",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "uniques: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[13] at reduceByKey at <console>:77\nres65: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[13] at reduceByKey at <console>:77\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ShuffledRDD[13] at reduceByKey at &lt;console&gt;:77"
      },
      "output_type" : "execute_result",
      "execution_count" : 39,
      "time" : "Took: 934 milliseconds, at 2017-5-22 17:25"
    } ]
  }, {
    "metadata" : {
      "id" : "340390D700F34A198E099DF39FBBC74D"
    },
    "cell_type" : "markdown",
    "source" : "In SQL you would use `GROUP BY` for this (including SQL queries you might write with Spark's [Dataset/DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html) API). However, in the `RDD` API, this is too expensive for our needs, because we don't care about the groups themselves, the long list of repeated `(word,fileName)` pairs. We only care about how many elements are in each group, that is their _size_. That's the purpose of the `1` in the tuples and the use of `RDD.reduceByKey`. It brings together all records with the same key, the unique `(word,fileName)` pairs, and then applies the anonymous function to \"reduce\" the values, the `1`s. I simply sum them up to compute the group counts.\n\nNote that the anonymous function `reduceByKey` expects must take two arguments, so I need parentheses around the argument list. Since this function fits on the same line, I used parentheses for `reduceByKey`, instead of braces.\n\n> **Note:** All the `*ByKey` methods operate on two-element tuples and treat the first element as the key, by default."
  }, {
    "metadata" : {
      "id" : "93409D472EBF4C07BD74EA733D4D6296"
    },
    "cell_type" : "markdown",
    "source" : "How many are there? Let's see a few:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "17D9B46320474C17BD71A9EDA0F717E9"
    },
    "cell_type" : "code",
    "source" : "uniques.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res67: Long = 27276\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "27276"
      },
      "output_type" : "execute_result",
      "execution_count" : 40,
      "time" : "Took: 1 second 618 milliseconds, at 2017-5-22 17:25"
    } ]
  }, {
    "metadata" : {
      "id" : "DC87537AD03041868BF85C6645D4B72D"
    },
    "cell_type" : "markdown",
    "source" : "As you would expect from a `GROUP BY`-like statement, the number of records is smaller than before. There are about 1/6 as many records now, meaning that on average, each `(word,fileName)` combination appears 6 times."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "15493734ED804BC484BF3DF21DEBDF6F"
    },
    "cell_type" : "code",
    "source" : "uniques.take(30).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "((dexterity,merrywivesofwindsor),1)\n((crest,asyoulikeit),1)\n((whole,comedyoferrors),2)\n((lamb,muchadoaboutnothing),2)\n((force,muchadoaboutnothing),2)\n((letter,merrywivesofwindsor),19)\n((blunt,tamingoftheshrew),3)\n((bestow,asyoulikeit),1)\n((rear,midsummersnightsdream),1)\n((crossing,tamingoftheshrew),1)\n((wronged,merrywivesofwindsor),4)\n((S,tamingoftheshrew),10)\n((HIPPOLYTA,midsummersnightsdream),19)\n((revolve,twelfthnight),1)\n((er,merrywivesofwindsor),11)\n((renown,asyoulikeit),1)\n((cubiculo,twelfthnight),1)\n((All,twelfthnight),3)\n((power,loveslabourslost),8)\n((Albeit,asyoulikeit),1)\n((lips,tamingoftheshrew),3)\n((upshot,twelfthnight),1)\n((approach,midsummersnightsdream),4)\n((mean,muchadoaboutnothing),5)\n((embossed,asyoulikeit),1)\n((varnish,loveslabourslost),2)\n((Apollo,midsummersnightsdream),1)\n((spangled,midsummersnightsdream),1)\n((gentlemen,comedyoferrors),1)\n((Rebuke,loveslabourslost),1)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 43,
      "time" : "Took: 1 second 917 milliseconds, at 2017-5-22 17:28"
    } ]
  }, {
    "metadata" : {
      "id" : "883847887BD84C2C8AFF71A63A87638F"
    },
    "cell_type" : "markdown",
    "source" : "For _inverted index_, we want our final keys to be the words themselves, so let's restructure the tuples from `((word,fileName),count)` to `(word,(fileName,count))`. Now, I'll still output two-element, key-value tuples, but the `word` will be the key and the `(fileName,count)` tuple will be the value."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B945394B877C4C1D8C601E446D5A856B"
    },
    "cell_type" : "code",
    "source" : "val words = uniques.map { word_file_count_tup3 => \n    (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "words: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[15] at map at <console>:79\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 44,
      "time" : "Took: 1 second 359 milliseconds, at 2017-5-22 17:29"
    } ]
  }, {
    "metadata" : {
      "id" : "D2412BFCCAE942448B2F0F2A83755325"
    },
    "cell_type" : "markdown",
    "source" : "The nested tuple methods, e.g., `_1._2`, are hard to read, making the logic somewhat obscure. We'll see a beautiful and elegant alternative shortly."
  }, {
    "metadata" : {
      "id" : "8017FCDB61244ABF982FEBD7EDE2FE30"
    },
    "cell_type" : "markdown",
    "source" : "Now I'll use an actual `group by` operation, because I now need to retain the groups. Calling `RDD.groupByKey` uses the first tuple element, now just the `words`, to bring together all occurrences of the unique words. Next, I'll sort the result by word, ascending alphabetically."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "3A55E6B49D8E4E1D85F8DE4B3715F750"
    },
    "cell_type" : "code",
    "source" : "val wordGroups = words.groupByKey.sortByKey(ascending = true)\nwordGroups",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "wordGroups: org.apache.spark.rdd.RDD[(String, Iterable[(String, Int)])] = ShuffledRDD[19] at sortByKey at <console>:81\nres75: org.apache.spark.rdd.RDD[(String, Iterable[(String, Int)])] = ShuffledRDD[19] at sortByKey at <console>:81\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ShuffledRDD[19] at sortByKey at &lt;console&gt;:81"
      },
      "output_type" : "execute_result",
      "execution_count" : 45,
      "time" : "Took: 1 second 340 milliseconds, at 2017-5-22 17:33"
    } ]
  }, {
    "metadata" : {
      "id" : "EF406143D451401B938CC7CBC3102B54"
    },
    "cell_type" : "markdown",
    "source" : "Note that each group is actually a Scala [Iterable](http://www.scala-lang.org/api/current/index.html#scala.collection.Iterable), i.e., an abstraction for some sort of collection. (It's actually a Spark-defined, private collection type called a `CompactBuffer`.)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0591CBBB7ED04BC18313CC4B8DD11F22"
    },
    "cell_type" : "code",
    "source" : "wordGroups.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res77: Long = 11951\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "11951"
      },
      "output_type" : "execute_result",
      "execution_count" : 46,
      "time" : "Took: 1 second 601 milliseconds, at 2017-5-22 17:33"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0238CC44E40444DDB43D53ACCBBA6513"
    },
    "cell_type" : "code",
    "source" : "wordGroups.take(30).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(,CompactBuffer((tamingoftheshrew,1), (asyoulikeit,1), (merrywivesofwindsor,1), (comedyoferrors,1), (midsummersnightsdream,1), (twelfthnight,1), (loveslabourslost,1), (muchadoaboutnothing,1)))\n(A,CompactBuffer((loveslabourslost,78), (midsummersnightsdream,39), (muchadoaboutnothing,31), (merrywivesofwindsor,38), (comedyoferrors,42), (asyoulikeit,34), (twelfthnight,47), (tamingoftheshrew,59)))\n(ABOUT,CompactBuffer((muchadoaboutnothing,18)))\n(ACT,CompactBuffer((asyoulikeit,22), (comedyoferrors,11), (tamingoftheshrew,12), (loveslabourslost,9), (muchadoaboutnothing,17), (twelfthnight,18), (merrywivesofwindsor,23), (midsummersnightsdream,9)))\n(ADAM,CompactBuffer((asyoulikeit,16)))\n(ADO,CompactBuffer((muchadoaboutnothing,18)))\n(ADRIANA,CompactBuffer((comedyoferrors,85)))\n(ADRIANO,CompactBuffer((loveslabourslost,111)))\n(AEGEON,CompactBuffer((comedyoferrors,20)))\n(AEMELIA,CompactBuffer((comedyoferrors,16)))\n(AEMILIA,CompactBuffer((comedyoferrors,3)))\n(AEacides,CompactBuffer((tamingoftheshrew,1)))\n(AEgeon,CompactBuffer((comedyoferrors,7)))\n(AEgle,CompactBuffer((midsummersnightsdream,1)))\n(AEmilia,CompactBuffer((comedyoferrors,4)))\n(AEsculapius,CompactBuffer((merrywivesofwindsor,1)))\n(AGUECHEEK,CompactBuffer((twelfthnight,2)))\n(ALL,CompactBuffer((midsummersnightsdream,2), (tamingoftheshrew,2)))\n(AMIENS,CompactBuffer((asyoulikeit,16)))\n(ANDREW,CompactBuffer((twelfthnight,104)))\n(ANGELO,CompactBuffer((comedyoferrors,36)))\n(ANN,CompactBuffer((merrywivesofwindsor,1)))\n(ANNE,CompactBuffer((merrywivesofwindsor,27)))\n(ANTIPHOLUS,CompactBuffer((comedyoferrors,195)))\n(ANTONIO,CompactBuffer((muchadoaboutnothing,32), (twelfthnight,32)))\n(ARMADO,CompactBuffer((loveslabourslost,111)))\n(AS,CompactBuffer((asyoulikeit,24)))\n(AUDREY,CompactBuffer((asyoulikeit,18)))\n(Abate,CompactBuffer((midsummersnightsdream,1), (loveslabourslost,1)))\n(Abbess,CompactBuffer((comedyoferrors,2)))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 47,
      "time" : "Took: 1 second 323 milliseconds, at 2017-5-22 17:33"
    } ]
  }, {
    "metadata" : {
      "id" : "2A2C2E4A954A40718D3E9059EAE0D08C"
    },
    "cell_type" : "markdown",
    "source" : "Finally, let's clean up these `CompactBuffers`. Let's convert each to a Scala [Vector](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Vector) (a collection with _O(1)_ performance for most operations), then sort it _descending_ by count, so the locations that mention the corresponding word the _most_ appear _first_ in the list. (Think about how you would want a search tool to work...) \n\nNote we're using `Vector.sortBy`, not an `RDD` sorting method. It takes a function that accepts each collection element and returns something used to sort the collection. By returning `(-fileNameCountTuple2._2, fileNameCountTuple2)`, I effectively say, \"sort by the counts _descending_ first, then sort by the file names.\" Why does `-fileNameCountTuple2._2` cause counts to be sorted descending, because I'm returning the negative of the value, so larger counts will be less than smaller counts, e.g., `-3 < -2`.\n\nFinally, I take the resulting `Vector` and make a comma-separated string with the elements, using the helper method `mkString`.\n\nWhat's `RDD.mapValues`? I could use `RDD.map`, but I'm not changing the keys (the words), so rather than have to deal with the tuple with both elements, `mapValues` just passes in the value part of the tuple and reconstructs new `(key,value)` tuples with the new value that my function returns. So, `mapValues` is more convenient to use than `map` when I have two-element tuples and I'm not modifying the keys."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "016D0589F20E40B58212D382F9A2FEC8"
    },
    "cell_type" : "code",
    "source" : "val iiFirstPass2 = wordGroups.mapValues { iterable => \n    val vect = iterable.toVector.sortBy { file_count_tup2 => \n        (-file_count_tup2._2, file_count_tup2._1)\n    }\n    vect.mkString(\",\")\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iiFirstPass2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[20] at mapValues at <console>:83\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 48,
      "time" : "Took: 1 second 84 milliseconds, at 2017-5-22 17:35"
    } ]
  }, {
    "metadata" : {
      "id" : "7C7007C29F564BBB852779540111D12D"
    },
    "cell_type" : "markdown",
    "source" : "We're done! The number of records is the same as for `wordGroups` (do you understand why?), so let's just see see some of the records."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9F721446ECFC4A5A8A82ECC97D49BF17"
    },
    "cell_type" : "code",
    "source" : "iiFirstPass2.take(30).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(,(asyoulikeit,1),(comedyoferrors,1),(loveslabourslost,1),(merrywivesofwindsor,1),(midsummersnightsdream,1),(muchadoaboutnothing,1),(tamingoftheshrew,1),(twelfthnight,1))\n(A,(loveslabourslost,78),(tamingoftheshrew,59),(twelfthnight,47),(comedyoferrors,42),(midsummersnightsdream,39),(merrywivesofwindsor,38),(asyoulikeit,34),(muchadoaboutnothing,31))\n(ABOUT,(muchadoaboutnothing,18))\n(ACT,(merrywivesofwindsor,23),(asyoulikeit,22),(twelfthnight,18),(muchadoaboutnothing,17),(tamingoftheshrew,12),(comedyoferrors,11),(loveslabourslost,9),(midsummersnightsdream,9))\n(ADAM,(asyoulikeit,16))\n(ADO,(muchadoaboutnothing,18))\n(ADRIANA,(comedyoferrors,85))\n(ADRIANO,(loveslabourslost,111))\n(AEGEON,(comedyoferrors,20))\n(AEMELIA,(comedyoferrors,16))\n(AEMILIA,(comedyoferrors,3))\n(AEacides,(tamingoftheshrew,1))\n(AEgeon,(comedyoferrors,7))\n(AEgle,(midsummersnightsdream,1))\n(AEmilia,(comedyoferrors,4))\n(AEsculapius,(merrywivesofwindsor,1))\n(AGUECHEEK,(twelfthnight,2))\n(ALL,(midsummersnightsdream,2),(tamingoftheshrew,2))\n(AMIENS,(asyoulikeit,16))\n(ANDREW,(twelfthnight,104))\n(ANGELO,(comedyoferrors,36))\n(ANN,(merrywivesofwindsor,1))\n(ANNE,(merrywivesofwindsor,27))\n(ANTIPHOLUS,(comedyoferrors,195))\n(ANTONIO,(muchadoaboutnothing,32),(twelfthnight,32))\n(ARMADO,(loveslabourslost,111))\n(AS,(asyoulikeit,24))\n(AUDREY,(asyoulikeit,18))\n(Abate,(loveslabourslost,1),(midsummersnightsdream,1))\n(Abbess,(comedyoferrors,2))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 49,
      "time" : "Took: 1 second 323 milliseconds, at 2017-5-22 17:35"
    } ]
  }, {
    "metadata" : {
      "id" : "6FA89C6B99FA4A658F42A726DB7BBF9D"
    },
    "cell_type" : "markdown",
    "source" : "Okay. Looks reasonable. \n\nNext, I'll refine the code using a very powerful feature, _pattern matching_, which both makes the code more concise and easier to understand. It's my *favorite* feature of Scala."
  }, {
    "metadata" : {
      "id" : "D54E35935C124319897560C95FED8D8D"
    },
    "cell_type" : "markdown",
    "source" : "Before I do that, try a few refinements on your own.\n\n**Exercises:**\n\nTry the following exercises in the next cell, which is a copy of `iiFirstPast`:\n\n* Convert all words to lower case. Calling `toLowerCase` on a string is all you need. Where's a good place to insert this logic?\n* Add a filter statement to remove the first entry for the blank word \"\". You could do this one of two ways, using another \"step\" with [RDD.filter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) (search the [Scaladoc page]((http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) for the `filter` method), _or_ using the similar Scala collections method, [scala.collection.Seq.filter](http://www.scala-lang.org/api/current/index.html#scala.collection.Seq). Both versions take a _predicate_ function, one that returns `true` if the record should be _retained_ and `false` otherwise. Do you think one choice is better than the other? Why? Or, are they basically the same? Reasons might include code comprehension and performance of one over the other.\n\nI'll implement both changes in subsequent refinements below."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F8DFDE55DFC542758E4869AB319A3B81"
    },
    "cell_type" : "code",
    "source" : "val iiFirstPass2 = sc.wholeTextFiles(shakespeare.toString).\n    flatMap { location_contents_tuple2 => \n        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n        words.map(word => ((word, fileName), 1))\n    }.\n    reduceByKey((count1, count2) => count1 + count2).\n    map { word_file_count_tup3 => \n        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n    }.\n    groupByKey.  // (word, iterable(...))\n    sortByKey(ascending = true).\n\n    mapValues { iterable => \n        val vect = iterable.toVector.sortBy { file_count_tup2 => \n            (-file_count_tup2._2, file_count_tup2._1)\n        }\n        vect.mkString(\",\")\n    }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iiFirstPass2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[30] at mapValues at <console>:94\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 50,
      "time" : "Took: 2 seconds 92 milliseconds, at 2017-5-22 17:37"
    } ]
  }, {
    "metadata" : {
      "id" : "6EA7FC67FF83467BAEA808BD865A6EF5"
    },
    "cell_type" : "markdown",
    "source" : "Let's try it!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "99F32FA708704775B8E7F05E3F9C2255"
    },
    "cell_type" : "code",
    "source" : "iiFirstPass2.take(30).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(,(asyoulikeit,1),(comedyoferrors,1),(loveslabourslost,1),(merrywivesofwindsor,1),(midsummersnightsdream,1),(muchadoaboutnothing,1),(tamingoftheshrew,1),(twelfthnight,1))\n(A,(loveslabourslost,78),(tamingoftheshrew,59),(twelfthnight,47),(comedyoferrors,42),(midsummersnightsdream,39),(merrywivesofwindsor,38),(asyoulikeit,34),(muchadoaboutnothing,31))\n(ABOUT,(muchadoaboutnothing,18))\n(ACT,(merrywivesofwindsor,23),(asyoulikeit,22),(twelfthnight,18),(muchadoaboutnothing,17),(tamingoftheshrew,12),(comedyoferrors,11),(loveslabourslost,9),(midsummersnightsdream,9))\n(ADAM,(asyoulikeit,16))\n(ADO,(muchadoaboutnothing,18))\n(ADRIANA,(comedyoferrors,85))\n(ADRIANO,(loveslabourslost,111))\n(AEGEON,(comedyoferrors,20))\n(AEMELIA,(comedyoferrors,16))\n(AEMILIA,(comedyoferrors,3))\n(AEacides,(tamingoftheshrew,1))\n(AEgeon,(comedyoferrors,7))\n(AEgle,(midsummersnightsdream,1))\n(AEmilia,(comedyoferrors,4))\n(AEsculapius,(merrywivesofwindsor,1))\n(AGUECHEEK,(twelfthnight,2))\n(ALL,(midsummersnightsdream,2),(tamingoftheshrew,2))\n(AMIENS,(asyoulikeit,16))\n(ANDREW,(twelfthnight,104))\n(ANGELO,(comedyoferrors,36))\n(ANN,(merrywivesofwindsor,1))\n(ANNE,(merrywivesofwindsor,27))\n(ANTIPHOLUS,(comedyoferrors,195))\n(ANTONIO,(muchadoaboutnothing,32),(twelfthnight,32))\n(ARMADO,(loveslabourslost,111))\n(AS,(asyoulikeit,24))\n(AUDREY,(asyoulikeit,18))\n(Abate,(loveslabourslost,1),(midsummersnightsdream,1))\n(Abbess,(comedyoferrors,2))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 51,
      "time" : "Took: 1 second 408 milliseconds, at 2017-5-22 17:38"
    } ]
  }, {
    "metadata" : {
      "id" : "BC161EB30C3B4F4EA5E341E88523BE8C"
    },
    "cell_type" : "markdown",
    "source" : "## Pattern Matching\nWe've studied a real program and we've learned quite a bit of Scala. Let's improve it with my favorite Scala feature, _pattern matching_.\n\nHere's the \"first pass\" version again for easy reference."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DA1A70091E8E4407A223F8AB9E2F1E62"
    },
    "cell_type" : "code",
    "source" : "val iiFirstPass3 = sc.wholeTextFiles(shakespeare.toString).\n    flatMap { location_contents_tuple2 => \n        val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n        val fileName = location_contents_tuple2._1.split(pathSeparator).last\n        words.map(word => ((word, fileName), 1))\n    }.\n    reduceByKey((count1, count2) => count1 + count2).\n    map { word_file_count_tup3 => \n        (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n    }.\n    groupByKey.\n    sortByKey(ascending = true).\n    mapValues { iterable => \n        val vect = iterable.toVector.sortBy { file_count_tup2 => \n            (-file_count_tup2._2, file_count_tup2._1)\n        }\n        vect.mkString(\",\")\n    }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iiFirstPass3: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[40] at mapValues at <console>:93\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 52,
      "time" : "Took: 2 seconds 99 milliseconds, at 2017-5-22 17:40"
    } ]
  }, {
    "metadata" : {
      "id" : "B776C7F5F9E04F5B8D15EEEA2B1CDAC2"
    },
    "cell_type" : "markdown",
    "source" : "Now here is a new implementation that uses _pattern matching_. \n\nI've also made two other additions, the solutions to the last exercises, which remove empty words \"\" and fix mixed capitalization, using the following additions:\n* `filter(word => word.size > 0)` to remove the empty words. (In Spark and Scala collections, `filter` has the positive sense; what should be retained?) It's indicated by the comment `// #1`.\n* `word.toLowerCase` to convert all words to lower case uniformly, so that words like HAMLET, Hamlet, and hamlet in the original texts are treated as the same, since we're counting word occurrences. See comment `// #2`."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4AC1B06955A74550AF9304053604DEEC"
    },
    "cell_type" : "code",
    "source" : "val iiPatternMatching = sc.wholeTextFiles(shakespeare.toString).\n  flatMap {\n        case (location, contents) => \n            val words = contents.split(\"\"\"\\W+\"\"\").\n                filter(word => word.size > 0)                      // #1\n            val fileName = location.split(pathSeparator).last\n            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n    }.\n    reduceByKey((count1, count2) => count1 + count2).\n    map { \n        case ((word, fileName), count) => (word, (fileName, count)) \n    }.\n    groupByKey.\n    sortByKey(ascending = true).\n    mapValues { iterable => \n        val vect = iterable.toVector.sortBy { \n            case (fileName, count) => (-count, fileName) \n        }\n        vect.mkString(\",\")\n    }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iiPatternMatching: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[50] at mapValues at <console>:95\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 53,
      "time" : "Took: 1 second 802 milliseconds, at 2017-5-22 17:40"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B62E8C2FDBFD46B49A3A1FFC7CFA02AC"
    },
    "cell_type" : "code",
    "source" : "iiPatternMatching.take(10).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(a,(loveslabourslost,507),(merrywivesofwindsor,494),(muchadoaboutnothing,492),(asyoulikeit,461),(tamingoftheshrew,445),(twelfthnight,416),(midsummersnightsdream,281),(comedyoferrors,254))\n(abandon,(asyoulikeit,4),(tamingoftheshrew,1),(twelfthnight,1))\n(abate,(loveslabourslost,1),(midsummersnightsdream,1),(tamingoftheshrew,1))\n(abatement,(twelfthnight,1))\n(abbess,(comedyoferrors,8))\n(abbey,(comedyoferrors,9))\n(abbominable,(loveslabourslost,1))\n(abbreviated,(loveslabourslost,1))\n(abed,(asyoulikeit,1),(twelfthnight,1))\n(abetting,(comedyoferrors,1))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 54,
      "time" : "Took: 1 second 306 milliseconds, at 2017-5-22 17:40"
    } ]
  }, {
    "metadata" : {
      "id" : "667841BDEC2144649AB392459097729A"
    },
    "cell_type" : "markdown",
    "source" : "There are more things we can do with pattern matching. We'll return to it below."
  }, {
    "metadata" : {
      "id" : "8881B4F7C5914C4380FCE7C450174D29"
    },
    "cell_type" : "markdown",
    "source" : "Let's understand this code. First, compare how I did the filtering for empty words and conversion to lower case with your exercise solutions above. Inside the function passed to `flatMap`, I filtered for empty words immediately after splitting the contents into words and I converted to lower case as I constructed the new tuples. My choice reduces the number of output records from `flatMap` by at most one record per input line, which shouldn't have a significant impact on performance. Filtering itself adds some extra overhead. \n\nAlso, the way Spark implements steps like `map`, `flatMap`, `filter`, it would incur about the same overhead if I added an `RDD.filter` step instead. Note that we could also do the filtering later in the pipeline, after `groupByKey`, for example. So, whichever approach you implemented above is probably fine. You could do performance profiling of the different approaches, but you may not notice a significance difference until you use very large input data sets."
  }, {
    "metadata" : {
      "id" : "ED3DB6B5E11C456D802DCB3C78D9BCE0"
    },
    "cell_type" : "markdown",
    "source" : "The function I pass to `flatMap` now looks like this:\n\n```scala\nflatMap { \n    case (location, contents) => \n        val words = contents.split(\"\"\"\\W+\"\"\").\n            filter(word => word.size > 0)                      // #1\n        val fileName = location.split(pathSeparator).last\n        words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n}.\n```\n\nCompare it to the previous version (ignoring the enhancements for blank words and capitalization, marked with the \\#1 and \\#2 comments):\n\n```scala\nflatMap { location_contents_tuple2 => \n    val words = location_contents_tuple2._2.split(\"\"\"\\W+\"\"\")\n    val fileName = location_contents_tuple2._1.split(pathSeparator).last\n    words.map(word => ((word, fileName), 1))\n}.\n```    \n\nInstead of `location_contents_tuple2` a variable name for the whole tuple, I wrote `case (location, contents)`. The `case` keyword says I want to _pattern match_ on the object passed to the function. If it's a two-element tuple (and I know it always will be in this case), then _extract_ the first element and assign it to a variable named `location` and extract the second element and assign it to a variable named `contents`.\n\nNow, instead of accessing the location and content with the slighly obscure and verbose `location_contents_tuple2._1` and `location_contents_tuple2._2`, respectively, I use meaningful names, `location` and `contents`. The code becomes more concise and more readable. \n\nThis special function syntax is called a _partial function_. It's partial in the sense that it might only handle some inputs, not all possible inputs, which would make it _total_. Partial functions have the form:\n```scala\n{                             // Opening curly brace (you can't use parentheses here)\n  case pattern1 => body1      // First pattern and body. For multi-line bodies, put on next lines.\n  case pattern2 => body2      // Second pattern and body (optional).\n  ...                         // Optional additional patterns.\n}                             // Closing curly brace.\n```\nEach record will be tested against the patterns, in order, until a match is found. In our case, we know the format will be a two-element tuple, so `case (location, contents)` matches perfectly and only one `case` clause is required. If I fail to match any of the patterns, an exception is thrown."
  }, {
    "metadata" : {
      "id" : "F6C55F95192341F6844A0B38FB7FAA49"
    },
    "cell_type" : "markdown",
    "source" : "The `reduceByKey` step is unchanged:\n\n```scala\nreduceByKey((count1, count2) => count1 + count2).\n```\n\nTo be clear, this isn't a pattern-matching expression; there is no `case` keyword. It's just a \"regular\" function that takes two arguments, for the two things I'm adding."
  }, {
    "metadata" : {
      "id" : "2F1C2C9070EA40978BEBDA56AB35E0C2"
    },
    "cell_type" : "markdown",
    "source" : "My favorite improvement is the next line:\n\n```scala\nmap { \n    case ((word, fileName), count) => (word, (fileName, count)) \n}.\n```\n\nCompare it to the previous, obscure version:\n\n```\nmap { word_file_count_tup3 => \n    (word_file_count_tup3._1._1, (word_file_count_tup3._1._2, word_file_count_tup3._2)) \n}.\n```\n\nThe new implementation makes it clear what I'm doing; just shifting parentheses! That's all it takes to go from the `(word, fileName)` keys with `count` values to `word` keys and `(fileName, count)` values. Note that pattern matching works just fine with nested structures, like `((word, fileName), count)`.\n\nI hope you can appreciate how elegant and concise this expression is! Note how I thought of the next transformation I needed to do in preparation for the final group-by, to switch from `((word, fileName), count)` to `(word, (fileName, count))` and _I just wrote it down exactly as I pictured it!_\n\nCode like this makes writing Scala Spark code a sublime experience for me. I hope it will for you, too ;)"
  }, {
    "metadata" : {
      "id" : "7CE480B558D4474487CFA6D9C9998E65"
    },
    "cell_type" : "markdown",
    "source" : "The next two expressions are unchanged:\n\n```scala\ngroupByKey.\nsortByKey(ascending = true).\n```"
  }, {
    "metadata" : {
      "id" : "37961770ADD046038ED932DA56C9352A"
    },
    "cell_type" : "markdown",
    "source" : "The final `mapValues` now uses pattern matching to sort the `Vector` in each record:\n\n```scala\nmapValues { iterable => \n    val vect = iterable.toVector.sortBy { \n        case (fileName, count) => (-count, fileName) \n    }\n    vect.mkString(\",\")\n}\n```\n\nCompared to the original version, it's again easier to read:\n\n```scala\nmapValues { iterable => \n    val vect = iterable.toVector.sortBy { file_count_tup2 => \n        (-file_count_tup2._2, file_count_tup2._1)\n    }\n    vect.mkString(\",\")\n}\n```\n\nThe function I pass to `sortBy` returns a tuple used for sorting, with `-count` to force _descending_ numerical sort (biggest first) and `fileName` to secondarily sort by the file name, for equivalent counts. I could ignore file name order and just return `-count` (not a tuple). However, if you need more repeatable output in a distributed system like Spark, say for example to use in unit test validation, then the secondary sorting by file name is handy."
  }, {
    "metadata" : {
      "id" : "C132921850FE48A28107BF629F5AC29F"
    },
    "cell_type" : "markdown",
    "source" : "Compare this code to `iiFirstPass3`. It is much clearer to write just the constituents and the literal representations of their nested structures in parentheses, rather than using tuple variable with accessors, like `_1` and `_2`."
  }, {
    "metadata" : {
      "id" : "7BACC99351F546B98125B1813C3D1519"
    },
    "cell_type" : "markdown",
    "source" : "## Final Implementation and Using DataFrames"
  }, {
    "metadata" : {
      "id" : "AEFCD90AF8BF4D9C8944AAAC966D885D"
    },
    "cell_type" : "markdown",
    "source" : "Let's verify we still get reasonable results. Let's also transition to Spark's [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) API for its convenient display options. `DataFrames` are part of [Spark SQL](http://spark.apache.org/docs/latest/sql-programming-guide.html), as is [SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession). We can create a `DataFrame` from our `RDD` using the `SparkSession`. In Spark 1.X, you would create a [SQLContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext) instead. `SQLContext` is still available, but deprecated."
  }, {
    "metadata" : {
      "id" : "DF19D2277A81408F831B7715AC5A62CD"
    },
    "cell_type" : "markdown",
    "source" : "Now convert the `RDD` to a `DataFrame` with `spark.createDataFrame`, then use `toDF` (convert to another `DataFrame`?) with new names for each \"column\". (If you want to back port this code to Spark 1.6.X, either rename `spark` to `sqlContext`, the conventional name for a `SQLContext` instance, or just use the name `spark` for the instance.)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "5E1532F5A88042F685A3E759E749B2E1"
    },
    "cell_type" : "code",
    "source" : "val iiPatternMatchingDF = sparkSession.createDataFrame(iiPatternMatching).toDF(\"word\", \"locations_counts\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iiPatternMatchingDF: org.apache.spark.sql.DataFrame = [word: string, locations_counts: string]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 55,
      "time" : "Took: 3 seconds 444 milliseconds, at 2017-5-22 17:42"
    } ]
  }, {
    "metadata" : {
      "id" : "D047BDFA0EC64794865588169B7BB8D4"
    },
    "cell_type" : "markdown",
    "source" : "If we ask for the value of a DataFrame, Spark Notebook renders it with a paged, table view."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "92EAFF4259D84EAD8076DFE88562A1ED"
    },
    "cell_type" : "code",
    "source" : "iiPatternMatchingDF",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res92: org.apache.spark.sql.DataFrame = [word: string, locations_counts: string]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon1b614fe9cfc1325927865840f44c04bd&quot;,&quot;partitionIndexId&quot;:&quot;anon774f23c3bfb1af53427c3b0889a9427e&quot;,&quot;numPartitions&quot;:417,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;word&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;locations_counts&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 56,
      "time" : "Took: 3 seconds 462 milliseconds, at 2017-5-22 17:43"
    } ]
  }, {
    "metadata" : {
      "id" : "F727F2C7B2F947E483DFC84BE7E9C429"
    },
    "cell_type" : "markdown",
    "source" : "You could also use the `show` command, especially when using the `spark-shell`. It has two optional arguments:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "C401CCFF8B244318A035975A6AFC41B6"
    },
    "cell_type" : "code",
    "source" : "iiPatternMatchingDF.show(truncate=false, numRows=5)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|word     |locations_counts                                                                                                                                                                       |\n+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|a        |(loveslabourslost,507),(merrywivesofwindsor,494),(muchadoaboutnothing,492),(asyoulikeit,461),(tamingoftheshrew,445),(twelfthnight,416),(midsummersnightsdream,281),(comedyoferrors,254)|\n|abandon  |(asyoulikeit,4),(tamingoftheshrew,1),(twelfthnight,1)                                                                                                                                  |\n|abate    |(loveslabourslost,1),(midsummersnightsdream,1),(tamingoftheshrew,1)                                                                                                                    |\n|abatement|(twelfthnight,1)                                                                                                                                                                       |\n|abbess   |(comedyoferrors,8)                                                                                                                                                                     |\n+---------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 5 rows\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 57,
      "time" : "Took: 1 second 513 milliseconds, at 2017-5-22 17:43"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8155E017145448248295E0EDB873D20D"
    },
    "cell_type" : "code",
    "source" : "iiPatternMatchingDF.printSchema",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- word: string (nullable = true)\n |-- locations_counts: string (nullable = true)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 58,
      "time" : "Took: 977 milliseconds, at 2017-5-22 17:43"
    } ]
  }, {
    "metadata" : {
      "id" : "33384D6241B24F2B9B36FD0804EACD4B"
    },
    "cell_type" : "markdown",
    "source" : "## Our Final Version: Supporting SQL Queries\nTo play with some more Spark, let's write SQL queries to explore the resulting data. \n\nTo do this, let's first refine the output. Instead of creating a string for the list of `(location,count)` pairs, which is opaque to our SQL schema (i.e., just a string), let's \"unzip\" the collection into two arrays, one for the `locations` and one for the `counts`. That way, if we ask for the first element of each array, we'll have nicely separate fields that work better with Spark SQL queries.\n\n\"Zipping\" and \"unzipping\" work like a mechanical zipper. If I have a collection of tuples, say `List[(String, Int)]`, I convert this single collection of \"zippered\" values into two collections (in a tuple) of single values, `(List[String], List[Int])`. Zipping is the inverse operation."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D2B006BA32B149FD89B675A58CABDA97"
    },
    "cell_type" : "code",
    "source" : "val x = Seq(\"one\", \"two\", \"three\")\nval y = Seq(1, 2, 3)\nval xy = x.zip(y)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "x: Seq[String] = List(one, two, three)\ny: Seq[Int] = List(1, 2, 3)\nxy: Seq[(String, Int)] = List((one,1), (two,2), (three,3))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 59,
      "time" : "Took: 3 seconds 70 milliseconds, at 2017-5-22 19:29"
    } ]
  }, {
    "metadata" : {
      "id" : "80DB808FBC844215BC6895E2E125116F"
    },
    "cell_type" : "markdown",
    "source" : "Note the type signature of `xy` above. Now note the type and result of calling `unzip`."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "073A6205836A466FB9623A7D549D33F5"
    },
    "cell_type" : "code",
    "source" : "xy.unzip",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res99: (Seq[String], Seq[Int]) = (List(one, two, three),List(1, 2, 3))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "(List(one, two, three),List(1, 2, 3))"
      },
      "output_type" : "execute_result",
      "execution_count" : 60,
      "time" : "Took: 1 second 623 milliseconds, at 2017-5-22 19:29"
    } ]
  }, {
    "metadata" : {
      "id" : "C224719DD3084F0A9D81A85E693E40D1"
    },
    "cell_type" : "markdown",
    "source" : "Here is our final implementation, `ii1` rewritten with this change."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "120CBF8222CD4483BB8DD4EBDC7A3ED5"
    },
    "cell_type" : "code",
    "source" : "val ii = sc.wholeTextFiles(shakespeare.toString).\n    flatMap {\n        case (location, contents) => \n            val words = contents.split(\"\"\"\\W+\"\"\").\n                filter(word => word.size > 0)                      // #1\n            val fileName = location.split(pathSeparator).last\n            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n    }.\n    reduceByKey((count1, count2) => count1 + count2).\n    map { \n        case ((word, fileName), count) => (word, (fileName, count)) \n    }.\n    groupByKey.\n    sortByKey(ascending = true).\n    map {                         // Must use map now, because we'll format new records. \n      case (word, iterable) =>    // Hence, pattern match on the whole input record.\n\n        val vect = iterable.toVector.sortBy { \n            case (fileName, count) => (-count, fileName) \n        }\n\n        // Use `Vector.unzip`, which returns a single, two element tuple, where each\n        // element is a collection, one for the locations and one for the counts. \n        // I use pattern matching to extract these two collections into variables.\n        val (locations, counts) = vect.unzip  \n        \n        // Lastly, I'll compute the total count across all locations and return \n        // a new record with all four fields. The `reduceLeft` method takes a function\n        // that knows how to \"reduce\" the collection down to a final value, working \n        // from the left.\n        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)\n        \n        (word, totalCount, locations, counts)\n    }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "ii: org.apache.spark.rdd.RDD[(String, Int, scala.collection.immutable.Vector[String], scala.collection.immutable.Vector[Int])] = MapPartitionsRDD[69] at map at <console>:95\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 61,
      "time" : "Took: 3 seconds 968 milliseconds, at 2017-5-22 19:29"
    } ]
  }, {
    "metadata" : {
      "id" : "DE48F43EDC0B4FE983B358F999D1F00F"
    },
    "cell_type" : "markdown",
    "source" : "We've changed the ending `mapValues` call to a `map` call, because we'll construct entirely new records, not just new values with the same keys. Hence the full records, two-element tuples are passed in, rather than just the values, so we'll pattern match on the tuple:\n\n\n```scala\n    map {                         // Must use map now, because we'll format new records.\n      case (word, iterable) =>    // Hence, pattern match on the whole input record.\n\n        val vect = iterable.toVector.sortBy { \n            case (fileName, count) => (-count, fileName) \n        }\n```\n\n\nWe have a `Vector[String, Int]` of two-element tuples `(fileName, count)`. We use `Vector.unzip` to create a single, two element tuple, where each element is now a collection, one for the locations and one for the counts. The type is `(Vector[String], Vector[Int])`.\n\nWe can also use pattern matching with assignment! We immediately decompose the two-element tuple:\n\n```scala\n        // I use pattern matching to extract these two collections into variables.\n        val (locations, counts) = vect.unzip  \n```\n\nFinally, it's convenient to know how many locations and counts we have, so we'll compute another new column for the their count and format a four-element tuple as the final output.\n\n```scala\n        // Lastly, I'll compute the total count across all locations and return \n        // a new record with all four fields. The `reduceLeft` method takes a function\n        // that knows how to \"reduce\" the collection down to a final value, working \n        // from the left.\n        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)\n\n        (word, totalCount, locations, counts)\n    }\n```    "
  }, {
    "metadata" : {
      "id" : "1B97941817A049CAA7F89263D1D8BFE6"
    },
    "cell_type" : "markdown",
    "source" : "Okay! Now let's create a [DataFrame](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) with this data. The `toDF` method just returns the same `DataFrame`, but with appropriate names for the columns, instead of the synthesized names that `createDataFrame` generates (e.g., `_c1`, `_c2`, etc.)\n\nCaching the `DataFrame` in memory prevents Spark from recomputing `ii` from the input files _every time_ I write a query!\n\nFinally, to use SQL, I need to \"register\" a temporary table."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "ACACA5AC5DF9489CA03EFE559847B9D5"
    },
    "cell_type" : "code",
    "source" : "val iiDF = sparkSession.createDataFrame(ii).toDF(\"word\", \"total_count\", \"locations\", \"counts\")\niiDF.cache\niiDF.createOrReplaceTempView (\"inverted_index\")  // Use registerTempTable for Spark 1.6.X",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iiDF: org.apache.spark.sql.DataFrame = [word: string, total_count: int ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 62,
      "time" : "Took: 4 seconds 1 millisecond, at 2017-5-22 19:42"
    } ]
  }, {
    "metadata" : {
      "id" : "B3E7EFA85D784E519ABA70036450AB3F"
    },
    "cell_type" : "markdown",
    "source" : "Let's remind ourselves of the schema:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A0FF2B70D76043318104BAD9ECF67BF5"
    },
    "cell_type" : "code",
    "source" : "iiDF.printSchema",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "root\n |-- word: string (nullable = true)\n |-- total_count: integer (nullable = true)\n |-- locations: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- counts: array (nullable = true)\n |    |-- element: integer (containsNull = false)\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 63,
      "time" : "Took: 1 second 415 milliseconds, at 2017-5-22 19:43"
    } ]
  }, {
    "metadata" : {
      "id" : "DD372976F9D44EC48FEB02697BF7AE74"
    },
    "cell_type" : "markdown",
    "source" : "The following SQL query extracts the top location by count for each word, as well as the total count across all locations for the word. The Spark SQL dialect supports Hive SQL syntax (plus extensions in Spark 2.X) for extracting elements from arrays, maps, and structs ([details](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-CollectionFunctions)). Here I access the first element (index zero) from each array, using bracket syntax, `[n]`. "
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B5A8A092E0C84E4E87112E0181CE38B5"
    },
    "cell_type" : "code",
    "source" : "val topLocations = sparkSession.sql(\"\"\"\nSELECT word, total_count, locations[0] AS top_location, counts[0] AS top_count\nFROM inverted_index \n\"\"\")\ntopLocations",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "topLocations: org.apache.spark.sql.DataFrame = [word: string, total_count: int ... 2 more fields]\nres106: org.apache.spark.sql.DataFrame = [word: string, total_count: int ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon2a2885e0450bf1330e4a058592a4e23b&quot;,&quot;partitionIndexId&quot;:&quot;anonda136dcd7393553a1cd12e563a051392&quot;,&quot;numPartitions&quot;:417,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;word&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;total_count&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;top_location&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;top_count&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 64,
      "time" : "Took: 3 seconds 383 milliseconds, at 2017-5-22 19:43"
    } ]
  }, {
    "metadata" : {
      "id" : "6C6F9CAE64B6461883CB25DCA0575F3F"
    },
    "cell_type" : "markdown",
    "source" : "**Exercises:** The next cell copies the previous query, as a starting point.\n\n* Change the query to return the top two locations and counts. \n* Try other queries. For example, find all occurrences of words like `love`, `hate`, etc.\n\nSee the <a href=\"#ExerciseSolutions\">Appendix</a> for some solutions."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "7AF28C5FA07B4F2794DB32A75B04B703"
    },
    "cell_type" : "code",
    "source" : "val topTwoLocations = sparkSession.sql(\"\"\"\nSELECT word, total_count, locations[0] AS top_location, counts[0] AS top_count\nFROM inverted_index \n\"\"\")\ntopTwoLocations",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "topTwoLocations: org.apache.spark.sql.DataFrame = [word: string, total_count: int ... 2 more fields]\nres108: org.apache.spark.sql.DataFrame = [word: string, total_count: int ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon988608f29bbe1a489fa127d2fe29752e&quot;,&quot;partitionIndexId&quot;:&quot;anonde6659884939e971b01aee604118cc0d&quot;,&quot;numPartitions&quot;:417,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;word&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;total_count&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;top_location&quot;,&quot;type&quot;:&quot;string&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}},{&quot;name&quot;:&quot;top_count&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:true,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 65,
      "time" : "Took: 1 second 435 milliseconds, at 2017-5-22 19:43"
    } ]
  }, {
    "metadata" : {
      "id" : "A23483F607EF40E680568094173D7F84"
    },
    "cell_type" : "markdown",
    "source" : "### Removing the \"Stop Words\"\nDid you notice that one record we saw above was for the word \"a\". Not very useful if you're using this data for text searching, _sentiment mining_, etc. So called _stop words_, like _a_, _an_, _the_, _he_, _she_, _it_, etc., could also be removed.\n\nRecall the `filter` logic I added to remove \"\", `word => word.size > 0`. I could replace it with `word => keep(word)`, where `keep` is a method that does any additional filtering I want, like removing stop words."
  }, {
    "metadata" : {
      "id" : "19CA443F73DC490A9373828770C659E9"
    },
    "cell_type" : "markdown",
    "source" : "**Exercise:**\n\n* Complete the implementation the following `keep(word: String):Boolean` method. It currently just returns false if the word is zero length. Change it to add a check to see if the word appears in a list of _stop words_. If so, also return false. Hard code a short list of your own words (e.g., `the`, `a`, `he`, `she`, etc.). (See the <a href=\"#ExerciseSolutions\">Appendix</a> for the solution.)\n* \"Extra Credit\": Use a Spark [broadcast variable] for the `Set` of stop words.\n\nQuestion: why is a [Set](http://www.scala-lang.org/api/2.11.8/#scala.collection.Set) a good data structure to use for stop words?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "DAC11DF20581434781E25BCEF349003A"
    },
    "cell_type" : "code",
    "source" : "def keep(word: String): Boolean = word.size > 0",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "keep: (word: String)Boolean\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 66,
      "time" : "Took: 1 second 24 milliseconds, at 2017-5-22 19:43"
    } ]
  }, {
    "metadata" : {
      "id" : "0F872D97B6964F258A7D8DA77E2385B3"
    },
    "cell_type" : "markdown",
    "source" : "We call `keep` in the revised version of the program. See the `// here...` comment."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "97C71430BE424C44BEDD81A175FC6164"
    },
    "cell_type" : "code",
    "source" : "val iistop = sc.wholeTextFiles(shakespeare.toString).\n    flatMap {\n        case (location, contents) => \n            val words = contents.split(\"\"\"\\W+\"\"\").\n                filter(keep)                        // here. Recall you could also write filter(word => keep(word))\n            val fileName = location.split(pathSeparator).last\n            words.map(word => ((word.toLowerCase, fileName), 1))   // #2\n    }.\n    reduceByKey((count1, count2) => count1 + count2).\n    map { \n        case ((word, fileName), count) => (word, (fileName, count)) \n    }.\n    groupByKey.\n    sortByKey(ascending = true).\n    map {                         // Must use map now, because we'll format new records. \n      case (word, iterable) =>    // Hence, pattern match on the whole input record.\n\n        val vect = iterable.toVector.sortBy { \n            case (fileName, count) => (-count, fileName) \n        }\n\n        // Use `Vector.unzip`, which returns a single, two element tuple, where each\n        // element is a collection, one for the locations and one for the counts. \n        // I use pattern matching to extract these two collections into variables.\n        val (locations, counts) = vect.unzip  \n        \n        // Lastly, I'll compute the total count across all locations and return \n        // a new record with all four fields. The `reduceLeft` method takes a function\n        // that knows how to \"reduce\" the collection down to a final value, working \n        // from the left.\n        val totalCount = counts.reduceLeft((n1,n2) => n1+n2)\n        \n        (word, totalCount, locations, counts)\n    }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "iistop: org.apache.spark.rdd.RDD[(String, Int, scala.collection.immutable.Vector[String], scala.collection.immutable.Vector[Int])] = MapPartitionsRDD[95] at map at <console>:97\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 67,
      "time" : "Took: 2 seconds 381 milliseconds, at 2017-5-22 19:43"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "66CC12FD3ADB467E8EBFDBE473FB84C6"
    },
    "cell_type" : "code",
    "source" : "iistop.take(50).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(a,3350,Vector(loveslabourslost, merrywivesofwindsor, muchadoaboutnothing, asyoulikeit, tamingoftheshrew, twelfthnight, midsummersnightsdream, comedyoferrors),Vector(507, 494, 492, 461, 445, 416, 281, 254))\n(abandon,6,Vector(asyoulikeit, tamingoftheshrew, twelfthnight),Vector(4, 1, 1))\n(abate,3,Vector(loveslabourslost, midsummersnightsdream, tamingoftheshrew),Vector(1, 1, 1))\n(abatement,1,Vector(twelfthnight),Vector(1))\n(abbess,8,Vector(comedyoferrors),Vector(8))\n(abbey,9,Vector(comedyoferrors),Vector(9))\n(abbominable,1,Vector(loveslabourslost),Vector(1))\n(abbreviated,1,Vector(loveslabourslost),Vector(1))\n(abed,2,Vector(asyoulikeit, twelfthnight),Vector(1, 1))\n(abetting,1,Vector(comedyoferrors),Vector(1))\n(abhominable,1,Vector(loveslabourslost),Vector(1))\n(abhor,5,Vector(asyoulikeit, comedyoferrors, loveslabourslost, merrywivesofwindsor, muchadoaboutnothing),Vector(1, 1, 1, 1, 1))\n(abhors,2,Vector(twelfthnight),Vector(2))\n(abide,5,Vector(merrywivesofwindsor, midsummersnightsdream),Vector(3, 2))\n(abides,1,Vector(muchadoaboutnothing),Vector(1))\n(ability,2,Vector(muchadoaboutnothing, twelfthnight),Vector(1, 1))\n(abject,2,Vector(comedyoferrors, tamingoftheshrew),Vector(1, 1))\n(abjure,1,Vector(midsummersnightsdream),Vector(1))\n(abjured,2,Vector(tamingoftheshrew, twelfthnight),Vector(1, 1))\n(able,9,Vector(merrywivesofwindsor, midsummersnightsdream, asyoulikeit, comedyoferrors, tamingoftheshrew),Vector(4, 2, 1, 1, 1))\n(aboard,6,Vector(comedyoferrors, tamingoftheshrew),Vector(5, 1))\n(abode,1,Vector(tamingoftheshrew),Vector(1))\n(abominable,2,Vector(asyoulikeit, merrywivesofwindsor),Vector(1, 1))\n(abortive,1,Vector(loveslabourslost),Vector(1))\n(abound,1,Vector(midsummersnightsdream),Vector(1))\n(about,108,Vector(muchadoaboutnothing, merrywivesofwindsor, twelfthnight, tamingoftheshrew, asyoulikeit, comedyoferrors, midsummersnightsdream, loveslabourslost),Vector(35, 27, 10, 9, 7, 7, 7, 6))\n(above,22,Vector(merrywivesofwindsor, twelfthnight, tamingoftheshrew, loveslabourslost, asyoulikeit, comedyoferrors, muchadoaboutnothing),Vector(6, 6, 4, 3, 1, 1, 1))\n(abraham,2,Vector(merrywivesofwindsor),Vector(2))\n(abridgement,1,Vector(midsummersnightsdream),Vector(1))\n(abroad,3,Vector(loveslabourslost, tamingoftheshrew),Vector(2, 1))\n(abrogate,1,Vector(loveslabourslost),Vector(1))\n(abruptly,1,Vector(asyoulikeit),Vector(1))\n(absence,7,Vector(merrywivesofwindsor, asyoulikeit, comedyoferrors, loveslabourslost, midsummersnightsdream, twelfthnight),Vector(2, 1, 1, 1, 1, 1))\n(absent,5,Vector(asyoulikeit, muchadoaboutnothing, tamingoftheshrew, twelfthnight),Vector(2, 1, 1, 1))\n(absolute,1,Vector(merrywivesofwindsor),Vector(1))\n(abstinence,1,Vector(loveslabourslost),Vector(1))\n(abstract,1,Vector(merrywivesofwindsor),Vector(1))\n(abuse,3,Vector(merrywivesofwindsor, twelfthnight),Vector(2, 1))\n(abused,11,Vector(twelfthnight, asyoulikeit, comedyoferrors, loveslabourslost, merrywivesofwindsor, midsummersnightsdream, muchadoaboutnothing, tamingoftheshrew),Vector(4, 1, 1, 1, 1, 1, 1, 1))\n(abuses,2,Vector(asyoulikeit),Vector(2))\n(abusing,1,Vector(merrywivesofwindsor),Vector(1))\n(aby,2,Vector(midsummersnightsdream),Vector(2))\n(academe,1,Vector(loveslabourslost),Vector(1))\n(academes,2,Vector(loveslabourslost),Vector(2))\n(accent,5,Vector(loveslabourslost, asyoulikeit, midsummersnightsdream, twelfthnight),Vector(2, 1, 1, 1))\n(accept,5,Vector(tamingoftheshrew),Vector(5))\n(access,7,Vector(tamingoftheshrew, asyoulikeit, twelfthnight),Vector(5, 1, 1))\n(accidence,1,Vector(merrywivesofwindsor),Vector(1))\n(accident,2,Vector(muchadoaboutnothing, twelfthnight),Vector(1, 1))\n(accidentally,2,Vector(comedyoferrors, loveslabourslost),Vector(1, 1))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 65,
      "time" : "Took: 1 second 71 milliseconds, at 2017-3-21 17:6"
    } ]
  }, {
    "metadata" : {
      "id" : "53E07BA2AC3842E98DD0F66A5CF845EB"
    },
    "cell_type" : "markdown",
    "source" : "## More on Pattern Matching Syntax\nWe've only scratched the surface of pattern matching. Let's explore it some more.\n\nHere's another anonymous function using pattern matching that extends the previous function we passed to `flatMap`:\n\n```scala\n{\n    case (location, \"\") => \n        Array.empty[((String, String), Int)]  // Return an empty array\n    case (location, contents) => \n        val words = contents.split(\"\"\"\\W+\"\"\")\n        val fileName = location.split(pathSep).last\n        words.map(word => ((word, fileName), 1))\n}.\n```\n\nYou can have multiple `case` clauses, some of which might match on specific literal values (\"\" in this case) and others which are more general. The first case clause handles files with no content. The second clause is the same as before.\n\nPattern matching is _eager_. The first successful match in the order as written will win. If you reversed the order here, the `case (location, \"\")` would never match and the compiler would throw an \"unreachable code\" warning for it.\n\nNote that you don't have to put the lines after the `=>` inside braces, `{...}` (although you can). The `=>` and `case` keywords (or the final `}`) are sufficient to mark these blocks. Also, for a single-expression block, like the one for the first case clause, you can put the expression on the same line after the `=>` if you want (and it fits). \n\nFinally, if none of the case clauses matches, then a [MatchError](http://www.scala-lang.org/api/current/index.html#scala.MatchError) exception is thrown. In our case, we _always_ know we'll have two-element tuples, so the examples so far are fine. \n\nHere's a final contrived example to illustrate what's possible, using a sequence of objects of different types. First, we define a class for `Person`. We'll explain `case class` syntax later, but for now, just think of it as a simple Java class, where the argument list is automatically converted to fields of the class."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "45ECDDF504DA48A7A8530E553B9AE299"
    },
    "cell_type" : "code",
    "source" : "case class Person(name: String, age: Int)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class Person\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 68,
      "time" : "Took: 1 second 524 milliseconds, at 2017-5-22 19:44"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "A22422DA7F56403F83517E1DAD19D2A3"
    },
    "cell_type" : "code",
    "source" : "val stuff = Seq(1, 3.14159, 2L, 10, 4.4F, (\"one\", 1), (404F, \"boo\"), ((11, 12), 21, 31), \n               \"hello\", new Person(\"Dean\", 100))\n\nstuff.foreach {\n    case 10                   => println(s\"Found a 10\")\n    case i: Int               => println(s\"Found an Int:   $i\")\n    case l: Long              => println(s\"Found a Long:   $l\")\n    case f: Float             => println(s\"Found a Float:  $f\")\n    case d: Double            => println(s\"Found a Double: $d\")\n    case Person(name2, age2)  => println(s\"A person: $name2, $age2\")\n    case (x1, x2) => \n        println(s\"Found a two-element tuple with elements of arbitrary type: ($x1, $x2)\")\n    case ((x1a, x1b), _, x3) =>   // The \"_\" means match on this field, but then ignore it.\n        println(s\"Found a three-element tuple with 1st and 3th elements: ($x1a, $x1b), ignored field, and $x3\")\n    case default              => println(s\"Found something else: $default\")\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Found an Int:   1\nFound a Double: 3.14159\nFound a Long:   2\nFound a 10\nFound a Float:  4.4\nFound a two-element tuple with elements of arbitrary type: (one, 1)\nFound a two-element tuple with elements of arbitrary type: (404.0, boo)\nFound a three-element tuple with 1st and 3th elements: (11, 12), ignored field, and 31\nFound something else: hello\nA person: Dean, 100\nstuff: Seq[Any] = List(1, 3.14159, 2, 10, 4.4, (one,1), (404.0,boo), ((11,12),21,31), hello, Person(Dean,100))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 69,
      "time" : "Took: 1 second 767 milliseconds, at 2017-5-22 19:44"
    } ]
  }, {
    "metadata" : {
      "id" : "8BB870235FCB4763898579F65A6B293E"
    },
    "cell_type" : "markdown",
    "source" : "A few notes.\n* A literal like `1` is inferred to be `Int`, while `3.14159` is inferred to be `Double`. Add `L` or `F`, respectively, to infer `Long` or `Float` instead.\n* Note how we mixed specific type checking, e.g., `i: Int`, with more loosely-typed expressions, e.g., `(x1, x2)`, which expects a two-element tuple, but the element types are unconstrained.\n* All the words `i`, `l`, `f`, `d`, `x1`, `x2`, `x3`, and `default` are arbitrary variable names. Yes `default` is not a keyword, but an arbitrary choice for a variable name. We could use anything we want.\n* The last `default` clause specifies a variable with no type information. Hence, it matches _anything_, which is why this clause must appear last. This is the idiom to use when you aren't sure about the types of things you're matching against and you want to avoid a possible [MatchError](http://www.scala-lang.org/api/current/index.html#scala.MatchError).\n* If you want to match that something _exists_, but you don't need to bind it to a variable, then use `_`, as in the three-element tuple example.\n* The three-element tuple example also demonstrates that arbitrary nesting of expressions is supported, where the first element is expected to be a two-element tuple.\n\nAll the anonymous functions we've seen that use these pattern matching clauses have this format:\n\n```scala\n{ \n    case firstCase => ...\n    case secondCase => ...\n    ... \n}```\n\nThis format has a special name. It's called a _partial function_. All that means is that we only \"promise\" to accept arguments that match at least one of our `case` clauses, not any possible input. \n\nThe other kind of anonymous function we've seen is a _total function_, to be precise. \n\nRecall we said that for total functions you can use either `(...)` or `{...}` around them, depending on the \"look\" you want. For _partial functions_, you _must_ use `{...}`."
  }, {
    "metadata" : {
      "id" : "3987C83C80DB4EC18399001B6148F0E4"
    },
    "cell_type" : "markdown",
    "source" : "Also, recall that we used pattern matching with assignment, even with our case class `Person`!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "F0BEF4E415EA47088CA07F8BDFD44E5D"
    },
    "cell_type" : "code",
    "source" : "val (a, (b, (c1, _), d)) = (\"A\", (\"B\", (\"C1\", \"C2\"), \"D\"))\nprintln(s\" $a, $b, $c1, ?, $d\")\nval Person(name, _) = Person(\"Dean\", 29)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : " A, B, C1, ?, D\na: String = A\nb: String = B\nc1: String = C1\nd: String = D\nname: String = Dean\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 70,
      "time" : "Took: 1 second 70 milliseconds, at 2017-5-22 19:45"
    } ]
  }, {
    "metadata" : {
      "id" : "1EB1D204C073428E8226BDB65A013FF5"
    },
    "cell_type" : "markdown",
    "source" : "Try adding an `\"E\"` element to the tuple on the right-hand side, without changing the left-hand side. What happens? Try removing the `\"D\"` and `\"E\"` elements. What happens now?"
  }, {
    "metadata" : {
      "id" : "28CECBF1777C42E296EFB0CE1C6F858D"
    },
    "cell_type" : "markdown",
    "source" : "We'll come back to one last example of pattern matching when we discuss _case classes_."
  }, {
    "metadata" : {
      "id" : "DF36806F348A4FC0B7EA0E17824ACFCC"
    },
    "cell_type" : "markdown",
    "source" : "# \"Scala for Spark 102\"\nWe've covered a lot already in this notebook, focusing on the most important topics you need to know about Scala for daily use. Let's call them the \"Scala for Spark 101\" material.\n\nAt this point, I suggest you create a new notebook and play with Spark using what you've learned so far, then come back to this point if you run into something we didn't cover already. Chances are you're ready to learn the next bits of useful Scala, the \"102\" material."
  }, {
    "metadata" : {
      "id" : "F3356D8DF39442A6BED7894FC07F5D23"
    },
    "cell_type" : "markdown",
    "source" : "## What Runs Where?\nConsider Spark code like the following:\n```scala\nmyRDD.map { \n  case ((word, fileName), count) => (word, (fileName, count)) \n}\n```\nIt's completely unobvious where this code actually _runs_. In a typical compiled Scala program or the Scala interpreter, it would all run in the process of the program or interpreter. That's also true when you use local mode for Spark, but _not_ when you run in a cluster.\n\nLocally and in a cluster situation, your _driver_ program (including the Spark shell) runs the `myRDD.map` code immediately, which constructs a directed, acyclic graph (DAG) of processing steps to run _later_, when you ask for results. In other words, Spark is lazy and only runs the computation on demand. So, what happens to the function passed to `map` in this case? \n\nSpark serializes the function so it can be transmitted to nodes on the cluster for execution. Even in local mode it serializes the code, even though it will be executed in the same JVM process as your driver.\n\nUsually, all this is transparent to you, but sometimes you'll write a function that can't be serialized and you'll get a `NotSerializable` exception. Spark tries to tell you what part of the function isn't serializable, but usually your function references something outside, like a field in an enclosing class, and that pulls in data structures that can't be serialized. I won't discuss this issue further here, but you can search for longer explanations and workarounds."
  }, {
    "metadata" : {
      "id" : "BD1881613BF4402DA92CDC11B060B93A"
    },
    "cell_type" : "markdown",
    "source" : "## Scala's Object Model\nScala is a _hybrid_, object-oriented and functional programming language. The philosophy of Scala is that you exploit object orientation for encapsulation of details, i.e., _modularity_, but use functional programming for its logical precision when implementing those details. Most of what we've seen so far falls into the functional programming camp. Much of data manipulation and analysis is really Mathematics. Functional programming tries to stay close to how functions and values work in Mathematics.\n\nHowever, when writing non-trivial Spark programs, it's occasionally useful to exploit the object-oriented features."
  }, {
    "metadata" : {
      "id" : "4855076A7BC54D6C97D33A64EEFA0965"
    },
    "cell_type" : "markdown",
    "source" : "### Classes vs. Instances\nScala uses the same distinction between classes and instances that you find in Java. Classes are like _templates_ used to create instances. \n\nWe've talked about the _types_ of things, like `word` is a `String` and `totalCount` is an `Int`. A class defines a _type_ in the same sense.\n\nHere is an example class that we might use to represent the inverted index records we just created:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E669C6B01CFE4ECCA6697FE314017013"
    },
    "cell_type" : "code",
    "source" : "class IIRecord1(\n    word: String, \n    total_count: Int, \n    locations: Array[String], \n    counts: Array[Int]) {\n    \n    /** CSV formatted string, but use [a,b,c] for the arrays */\n    override def toString: String = {\n        val locStr = locations.mkString(\"[\", \",\", \"]\")  // i.e., \"[a,b,c]\"\n        val cntStr = counts.mkString(\"[\", \",\", \"]\")  // i.e., \"[1,2,3]\"\n        s\"$word,$total_count,$locStr,$cntStr\"\n    }\n}\n\nnew IIRecord1(\"hello\", 3, Array(\"one\", \"two\"), Array(1, 2))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class IIRecord1\nres116: IIRecord1 = hello,3,[one,two],[1,2]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "hello,3,[one,two],[1,2]"
      },
      "output_type" : "execute_result",
      "execution_count" : 71,
      "time" : "Took: 1 second 146 milliseconds, at 2017-5-22 19:47"
    } ]
  }, {
    "metadata" : {
      "id" : "258FF28F533148E9828A26183A19E9F4"
    },
    "cell_type" : "markdown",
    "source" : "When defining a class, the argument list after the class name is the argument list for the _primary constructor_. You can define secondary constructors, too, but it's not very common, in part for reasons we'll see shortly.\n\nNote that when you override a method that's defined in a parent class, like Java's `Object.toString`, Scala requires you to add the `override` keyword.\n\nWe created an _instance_ of `IIRecord1` using `new`, just like in Java.\n\nFinally, as a side note, we've been using `Ints` (integers) all along for the various counts, but really for \"big data\", we should probably use `Longs`."
  }, {
    "metadata" : {
      "id" : "1DAC760FBC7A4F239981F119549B58E4"
    },
    "cell_type" : "markdown",
    "source" : "### Objects\n\nI've been careful to use the word _instance_ for things we create from classes. That's because Scala has built-in support for the [Singleton Design Pattern](https://en.wikipedia.org/wiki/Singleton_pattern), i.e., when we only want one instance of a class. We use the `object` keyword. \n\nFor example, in Java, you define a class with a `static void main(String[] arguments)` method as your entry point into your program. In Scala, you use an `object` to hold `main`, as follows:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "238A29DAC4E54899BDF1E9154BA020C0"
    },
    "cell_type" : "code",
    "source" : "object MySparkJob {\n\n    val greeting = \"Hello Spark!\"\n    \n    def main(arguments: Array[String]) = {\n        println(greeting)\n        \n        // Create your SparkContext or SparkSession, etc., etc.\n    }\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined object MySparkJob\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 72,
      "time" : "Took: 1 second 482 milliseconds, at 2017-5-22 19:47"
    } ]
  }, {
    "metadata" : {
      "id" : "378CA4822C7A461A80EAE453340AA1B9"
    },
    "cell_type" : "markdown",
    "source" : "Just as for classes, the name of the object can be anything you want. There is no `static` keyword in Scala. Instead of adding `static` methods and fields to classes as in Java, you put them in objects instead, as here.\n\n> **NOTE:** Because the Scala compiler must generate valid JVM byte code, these definitions are converted into the equivalent, Java-like static definitions in the output byte code."
  }, {
    "metadata" : {
      "id" : "5AC7D2875B4C44F299F1ECF6E6A4674B"
    },
    "cell_type" : "markdown",
    "source" : "### Case Classes\nTuples are handy for representing records and for decomposing them with pattern matching. However, it would be nice if the fields were _named_, as well as _typed_. A good use for a class, like our `IIRecord1` above, us to represent this structure and give us named fields. Let's now refine that class definition to exploit some extra, very useful features in Scala.\n\nConsider the following definition of a _case class_ that represents our final record type."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "BFC725674989451788C183FD136D5FBC"
    },
    "cell_type" : "code",
    "source" : "case class IIRecord(\n    word: String, \n    total_count: Int = 0, \n    locations: Array[String] = Array.empty, \n    counts: Array[Int] = Array.empty) {\n\n    /** \n     * Different than our CSV output above, but see toCSV.\n     * Array.toString is useless, so format these ourselves. \n     */\n    override def toString: String = \n        s\"\"\"IIRecord($word, $total_count, $locStr, $cntStr)\"\"\"\n    \n    /** CSV-formatted string, but use [a,b,c] for the arrays */\n    def toCSV: String = \n        s\"$word,$total_count,$locStr,$cntStr\"\n        \n    /** Return a JSON-formatted string for the instance. */\n    def toJSONString: String = \n        s\"\"\"{\n        |  \"word\":        \"$word\", \n        |  \"total_count\": $total_count, \n        |  \"locations\":   ${toJSONArrayString(locations)},\n        |  \"counts\"       ${toArrayString(counts, \", \")}\n        |}\n        |\"\"\".stripMargin\n\n    private def locStr = toArrayString(locations)\n    private def cntStr = toArrayString(counts)\n\n    // \"[_]\" means we don't care what type of elements; we're just\n    // calling toString on them!\n    private def toArrayString(array: Array[_], delim: String = \",\"): String = \n        array.mkString(\"[\", delim, \"]\")  // i.e., \"[a,b,c]\"\n\n    private def toJSONArrayString(array: Array[String]): String =\n        toArrayString(array.map(quote), \", \")\n    \n    private def quote(word: String): String = \"\\\"\" + word + \"\\\"\"  \n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class IIRecord\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 73,
      "time" : "Took: 1 second 640 milliseconds, at 2017-5-22 19:47"
    } ]
  }, {
    "metadata" : {
      "id" : "E6F90E87F0FE4F0789FE3CBB009F2C84"
    },
    "cell_type" : "markdown",
    "source" : "I said that defining secondary constructors is not very common. In part, it's because I used a convenient feature, the ability to define default values for arguments to methods, including the primary constructor. The default values mean that I can create instances without providing all the arguments explicitly, as long as there is a default value defined, and similarly for calling methods. Consider these two examples:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D933A899DBEA4CFD886EA159A598CD23"
    },
    "cell_type" : "code",
    "source" : "val hello = new IIRecord(\"hello\")\nval world = new IIRecord(\"world!\", 3, Array(\"one\", \"two\"), Array(1, 2))\n\nprintln(\"\\n`toString` output:\")\nprintln(hello)\nprintln(world)\n\nprintln(\"\\n`toJSONString` output:\")\nprintln(hello.toJSONString)\nprintln(world.toJSONString)\n\nprintln(\"\\n`toCSV` output:\")\nprintln(hello.toCSV)\nprintln(world.toCSV)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "\n`toString` output:\nIIRecord(hello, 0, [], [])\nIIRecord(world!, 3, [one,two], [1,2])\n\n`toJSONString` output:\n{\n  \"word\":        \"hello\", \n  \"total_count\": 0, \n  \"locations\":   [],\n  \"counts\"       []\n}\n\n{\n  \"word\":        \"world!\", \n  \"total_count\": 3, \n  \"locations\":   [\"one\", \"two\"],\n  \"counts\"       [1, 2]\n}\n\n\n`toCSV` output:\nhello,0,[],[]\nworld!,3,[one,two],[1,2]\nhello: IIRecord = IIRecord(hello, 0, [], [])\nworld: IIRecord = IIRecord(world!, 3, [one,two], [1,2])\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 74,
      "time" : "Took: 1 second 984 milliseconds, at 2017-5-22 19:48"
    } ]
  }, {
    "metadata" : {
      "id" : "2EFE1FD2D6D5447886C2AD89CF017711"
    },
    "cell_type" : "markdown",
    "source" : "I added `toJSONString` to illustrate adding _public_ methods, the default visibility, and _private_ methods to a class definition. By the way, when there are no methods or non-field variables to define, I can omit the body complete; no empty `{}` required.\n\nRecall that the `override` keyword is required when redefining `toString`."
  }, {
    "metadata" : {
      "id" : "28D18FCD76A64DD592DD8BA3970CCFD5"
    },
    "cell_type" : "markdown",
    "source" : "Okay, what about that `case` keyword? It tells the compiler to do several useful things for us, eliminating a lot of boilerplate that we would have to write for ourselves with other languages, especially Java:\n\n1. Treat each constructor argument as an immutable (`val`) private field of the instance.\n1. Generate a public reader method for the field with the same name (e.g., `word`).\n1. Generate _correct_ implementations of the `equals` and `hashCode` methods, which people often implement incorrectly, as well as a default `toString` method. You can use your own definitions by adding them explicitly to the body. We did this for `toString`, to format the arrays in a nicer way than the default `Array[_].toString` method.\n1. Generate an `object IIRecord`, i.e., with the same name. The object is called the _companion object_.\n1. Generate a \"factory\" method in the companion object that takes the same argument list and instantiates an instance.\n1. Generate helper methods in the companion object that support pattern matching.\n\nPoints 1 and 2 make each argument behave as if they are public, read-only fields of the instance, but they are actually implemented as described.\n\nPoint 3 is important for correct behavior. Case class instances are often used as keys in [Maps](http://www.scala-lang.org/api/current/index.html#scala.collection.Map) and [Sets](http://www.scala-lang.org/api/current/index.html#scala.collection.Set), Spark RDD and DataFrame methods, etc. In fact, you should _only_ use your case classes or Scala's built-in types with well-defined `hashCode` and `equals` methods (like `Int` and other number types, `String`, tuples, etc.) as keys.\n\nFor point 4, the _companion object_ is generated automatically by the compiler. It adds the \"factory\" method discussed in point 5, and methods that support pattern matching, point 6. You can explicitly define these methods and others yourself, as well as fields to hold state. The compiler will still insert these other methods. However, see <a href=\"#Ambiguities\">Ambiguities with Companion Objects</a>. The bottom line is that you shouldn't define case classes in notebooks like this with extra methods in the companion object, due to parsing ambiguities.\n\nPoint 5 means you actually rarely use `new` when creating instances. That is, the following are effectively equivalent:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "4CFDB66B3462418F89967C3FF58F8222"
    },
    "cell_type" : "code",
    "source" : "val hello1 = new IIRecord(\"hello1\")\nval hello2 = IIRecord(\"hello2\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "hello1: IIRecord = IIRecord(hello1, 0, [], [])\nhello2: IIRecord = IIRecord(hello2, 0, [], [])\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 75,
      "time" : "Took: 1 second 598 milliseconds, at 2017-5-22 19:49"
    } ]
  }, {
    "metadata" : {
      "id" : "7472F705871D4CC38546FB8C4D416AAE"
    },
    "cell_type" : "markdown",
    "source" : "What actually happens in the second case, without `new`? The \"factory\" method is actually called `apply`. In Scala, whenever you put an argument list after any _instance_, including these `objects`, as in the `hello2` case, Scala looks for an `apply` method to call. The arguments have to match the argument list for apply (number of arguments, types of arguments, accounting for default argument values, etc.). Hence, the `hello2` declaration is really this:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "88DDD90C19B64C1C80C037DEEAA3E895"
    },
    "cell_type" : "code",
    "source" : "val hello2b = IIRecord.apply(\"hello2b\")",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "hello2b: IIRecord = IIRecord(hello2b, 0, [], [])\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 76,
      "time" : "Took: 869 milliseconds, at 2017-5-22 19:53"
    } ]
  }, {
    "metadata" : {
      "id" : "8FDF18BD8DA74C0D8CABD6EA3B96584A"
    },
    "cell_type" : "markdown",
    "source" : "You can exploit this feature, too, in your other classes. We talked about word stemming above. Suppose you write a stemming library and declare an object for as the entry point. Here, I'll just do something simple; assume a trailing \"s\" means the word is a plural and remove it (a bad assumption...):"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "E44A24511FC94B8F87974F7A3665FE7C"
    },
    "cell_type" : "code",
    "source" : "object stem {\n    def apply(word: String): String = word.replaceFirst(\"s$\", \"\") // insert real implementation!\n}\n\nprintln(stem(\"dog\"))\nprintln(stem(\"dogs\"))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dog\ndog\ndefined object stem\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 77,
      "time" : "Took: 1 second 381 milliseconds, at 2017-5-22 19:53"
    } ]
  }, {
    "metadata" : {
      "id" : "AFF82895E4174B9B8840C286B55E9942"
    },
    "cell_type" : "markdown",
    "source" : "Note how it looks like I'm calling a function or method named `stem`. Scala allows object and class names to start with a lower case letter."
  }, {
    "metadata" : {
      "id" : "7F9B448BBC2F47E6821CA916459FD59C"
    },
    "cell_type" : "markdown",
    "source" : "Finally, point 6 means we can use our custom case classes in pattern matching expressions, as we saw previously with a `Person` case class. I won't go into the methods actually implemented in the companion object and how they support pattern matching. I'll just use the \"magic\" in the following example that \"parses\" or previously-defined `hello` and `world` instances."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab1897802299-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "E5528BE4892B47AFBBA677D7A6E011E8"
    },
    "cell_type" : "code",
    "source" : "Seq(hello, world).map {\n    case IIRecord(word, 0, _, _) => s\"$word with no occurrences.\"\n    case IIRecord(word, cnt, locs, cnts) => \n        s\"$word occurs $cnt times: ${locs.zip(cnts).mkString(\", \")}\"\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res126: Seq[String] = List(hello with no occurrences., world! occurs 3 times: (one,1), (two,2))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon303895d78ee6cbb96a6a45104e0fdfc2&quot;,&quot;dataInit&quot;:[],&quot;genId&quot;:&quot;1897802299&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <div>\n        <ul class=\"nav nav-tabs\" id=\"ul1897802299\"><li>\n              <a href=\"#tab1897802299-0\"><i class=\"fa fa-table\"/></a>\n            </li><li>\n              <a href=\"#tab1897802299-1\"><i class=\"fa fa-cubes\"/></a>\n            </li></ul>\n\n        <div class=\"tab-content\" id=\"tab1897802299\"><div class=\"tab-pane\" id=\"tab1897802299-0\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon1b4e4efd9babd35886733ea71ffbe794&quot;,&quot;dataInit&quot;:[{&quot;string value&quot;:&quot;hello with no occurrences.&quot;},{&quot;string value&quot;:&quot;world! occurs 3 times: (one,1), (two,2)&quot;}],&quot;genId&quot;:&quot;257580499&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"string value\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon9a20eb9f86cfefff07d72f10e277f92d&quot;,&quot;initialValue&quot;:&quot;2&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon3f683d18c0d3b5394f996f03d7c201b3&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab1897802299-1\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon4924b6bca59262d4c21cbe2ce768952a&quot;,&quot;dataInit&quot;:[{&quot;string value&quot;:&quot;hello with no occurrences.&quot;},{&quot;string value&quot;:&quot;world! occurs 3 times: (one,1), (two,2)&quot;}],&quot;genId&quot;:&quot;1822646267&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon3426213d7ba0eeb17bf51814fa1f53f3&quot;,&quot;initialValue&quot;:&quot;2&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonf7db3782d96239f77a6ece7fb7689ec4&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div></div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 78,
      "time" : "Took: 1 second 128 milliseconds, at 2017-5-22 19:54"
    } ]
  }, {
    "metadata" : {
      "id" : "4FF0B17F9FDC430384E40A416C35D04E"
    },
    "cell_type" : "markdown",
    "source" : "The first case clause ignores the locations and counts, because I know they will be empty arrays if the total count is 0! \n\nThe second case clause uses the `zip` method to put the locations and counts back together. Recall we used `unzip` to create the separate collections."
  }, {
    "metadata" : {
      "id" : "B6FAD3997BF245C8AC6EDD4AE85DE15D"
    },
    "cell_type" : "markdown",
    "source" : "## Datasets and DataFrames\nWe've mostly used Spark's RDD API, where it's common to use case classes to represent the \"schema\" of records when working with RDDs, but also with a new type, [Dataset[T]](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), analogous to `RDD[T]`, where the `T` represents the type of records.\n\nA problem with [DataFrames](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrame) is the fact that the fields are untyped until you try to access them. `Datasets` restore the type safety of `RDDs` by using a case class as the definition of the schema. \n\n`Datasets` were introduced in Spark 1.6.0, but they were somewhat incomplete until the 2.0.0 release, where `Dataset[T]` is the  real type you work with when using SparkSQL. `DataFrame` is still around, but now it's a _type alias_ for `Dataset[Row]`, where [Row](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row) is the loosely-typed representation of the rows in the `Dataset` and its columns (or fields). \n\nA _type alias_ is another Scala feature where you can give a new name to a type expression. In this case: `type DataFrame = Dataset[Row]`. You use `DataFrame` as if it's declared as a regular type, but you are actually working with a `Dataset[Row]`."
  }, {
    "metadata" : {
      "id" : "0B4C288179C7414D8DE58D87168D6B35"
    },
    "cell_type" : "markdown",
    "source" : "### Importing Everything in a Package\nIn Java, `import foo.bar.*;` means import everything in the `bar` package.\n\nIn Scala, `*` is actually a legal method name; think of defining multiplication for custom numeric types, like `Matrix`. Hence, this import statement in Scala would be ambigious. Therefore, Scala uses `_` instead of `*`, `import foo.bar._` (with the semicolon inferred)."
  }, {
    "metadata" : {
      "id" : "8D557ECC0A1E47E4B58E4D2D8B9B0B29"
    },
    "cell_type" : "markdown",
    "source" : "Incidentally, what would that `*` method definition look like? Something like this:\n\n```scala\ncase class Matrix(rows: Array[Array[Double]]) {  // Each row is an Array[Double]\n\n    /** Multiple this matrix by another. */\n    def *(other: Matrix): Matrix = ...\n    \n    /** Add this matrix by another. */\n    def +(other: Matrix): Matrix = ...\n    \n    ...\n}\n\nval row1: Array[Array[Double]] = ...\nval row2: Array[Array[Double]] = ...\nval m1 = Matrix(rows1)\nval m2 = Matrix(rows2)\nval m1_times_m2 = m1 * m2\nval m1_plus_m2 = m1 + m2\n```"
  }, {
    "metadata" : {
      "id" : "D09A3AE2C3C3408F85322926C1AE71C7"
    },
    "cell_type" : "markdown",
    "source" : "### Operator Syntax\n\nWait!! What's this `m1 * m2` stuff?? Shouldn't it be `m1.*(m2)`. It would be really convenient to use \"operator syntax\", more precisely called _infix operator notation_ for many methods like `*` and `+` here. The Scala parser supports this with a simple relaxation of the rules; when a method takes a single argument, you can omit the period `.` and parentheses `(...)`. Hence the following really is equivalent:\n\n```scala\nval m1_times_m2 = m1.*(m2)\nval m1_times_m2 = m1 * m2\n```\n\nThis convenience can lead to confusing code, especially for beginners to Scala, so use it cautiously."
  }, {
    "metadata" : {
      "id" : "B7AF61B1BFC644BB8A2C39BBAB0AA04B"
    },
    "cell_type" : "markdown",
    "source" : "### Traits\n_Traits_ are similar to Java 8 _interfaces_, used to define abstractions, but with the ability to provide \"default\" implementations of the methods declared. Unlike Java 8 interfaces, traits can also have fields representing \"state\" information about instances. There is a blury line between traits and _abstract classes_, again where some member methods or fields are not defined. In both cases, a subtype of a trait and/or an abstract class must define any undefined members if you want to construct instances of it.\n\nSo, why have both traits and abstract classes? It's because Java only allows _single inheritance_; there can be only one _parent_ type, which is normally where you would use an abstract class, but Scala lets you \"mix in\" one or more additional traits (or use a trait as the parent class - yes, confusing). A great example \"mix in\" trait is one that implements logging. Any \"service\" type can mix in the logging trait to get \"instant\" access to this reusable functionality. Schematically, it looks like the following:\n\n```scala\n// Assume severity `Level` and `Logger` types defined elsewhere...\ntrait Logging {\n\n    def log(level: Level, message: String): Unit = logger.log(level, message)\n    \n    private logger: Logger = ...\n}\n\nabstract class Service {\n    def run(): Unit   // No body, so abstract!\n}\n\nclass MyService extends Service with Logging {\n    def run(): Unit = {\n        log(INFO, \"Staring MyService...\")\n        ...\n        log(INFO, \"Finished MyService\")\n    }\n}\n```\n\n`Unit` is Scala's equivalent to Java's `void`. It actually is a true type with a single return value, unlike `void`, but we use it in the same sense of \"nothing useful will be returned\"."
  }, {
    "metadata" : {
      "id" : "F4F9E4DFF1914C1A8CF2B359BC3A7142"
    },
    "cell_type" : "markdown",
    "source" : "### Ranges\nWhat if you want some numbers between a start and end value? Use a [Range](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Range), which has a nice literal syntax, e.g., `1 until 100`, `2 to 200 by 3`. \n\nThe `Range` always includes the lower bound. Using `to` in a `Range` makes it _inclusive_ at the upper bound. Using `until` makes it _exclusive_ at the upper bound. Use `by` to specify a delta, which defaults to `1`."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab2114654494-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "4094E87E9CBF4B7AB7813A84B31C3512"
    },
    "cell_type" : "code",
    "source" : "1 until 10",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res128: scala.collection.immutable.Range = Range(1, 2, 3, 4, 5, 6, 7, 8, 9)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon8c52f06ebc7b5d8d9572c5d8f6c258c4&quot;,&quot;dataInit&quot;:[],&quot;genId&quot;:&quot;2114654494&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <div>\n        <ul class=\"nav nav-tabs\" id=\"ul2114654494\"><li>\n              <a href=\"#tab2114654494-0\"><i class=\"fa fa-table\"/></a>\n            </li><li>\n              <a href=\"#tab2114654494-1\"><i class=\"fa fa-dot-circle-o\"/></a>\n            </li><li>\n              <a href=\"#tab2114654494-2\"><i class=\"fa fa-line-chart\"/></a>\n            </li><li>\n              <a href=\"#tab2114654494-3\"><i class=\"fa fa-bar-chart\"/></a>\n            </li><li>\n              <a href=\"#tab2114654494-4\"><i class=\"fa fa-cubes\"/></a>\n            </li></ul>\n\n        <div class=\"tab-content\" id=\"tab2114654494\"><div class=\"tab-pane\" id=\"tab2114654494-0\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anond1c2e4b295f43c72ba6aac0032e0c61e&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9}],&quot;genId&quot;:&quot;668070517&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"_1\",\"_2\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon73ab946d920d59dd1de50774c0e22a02&quot;,&quot;initialValue&quot;:&quot;9&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon2c12917a4e4591359e7b24011e2b88c7&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab2114654494-1\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon7dbcc72ec1d9604351ccc1d5a956c256&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9}],&quot;genId&quot;:&quot;406615945&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/scatterChart'], \n      function(playground, _magicscatterChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicscatterChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon122670876dc941080be366966866c7ca&quot;,&quot;initialValue&quot;:&quot;9&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anone4db32799f486a0a633ac09c07a611ce&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab2114654494-2\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon8fcfc91e0e1505941c1803a75fda90d6&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9}],&quot;genId&quot;:&quot;1328600161&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/lineChart'], \n      function(playground, _magiclineChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiclineChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon11b1e95f51b6f2c24beece4027c2a619&quot;,&quot;initialValue&quot;:&quot;9&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon4084b6ae25d3b91fee460227371dabda&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab2114654494-3\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon314475591b273aee2d7f1e404be8b9a1&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9}],&quot;genId&quot;:&quot;481850995&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/barChart'], \n      function(playground, _magicbarChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicbarChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon767ec1519f8a9243fe40fb080746bf19&quot;,&quot;initialValue&quot;:&quot;9&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon40d1d8f018d27e50dd4f837a6e86b5c3&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab2114654494-4\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonfd7858e9906cadea6bb4b240a0f1ff9f&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9}],&quot;genId&quot;:&quot;1528346141&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonfd009020c5ad96bc82025907b482d9dc&quot;,&quot;initialValue&quot;:&quot;9&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon7f580b31685360af71a5def187bb9764&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div></div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 79,
      "time" : "Took: 1 second 453 milliseconds, at 2017-5-22 19:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab352655552-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "DABC04E31BF74B9FAD7630F87CF197F3"
    },
    "cell_type" : "code",
    "source" : "1 to 10",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res130: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonb57a6d54ced77fda23627d625852e08a&quot;,&quot;dataInit&quot;:[],&quot;genId&quot;:&quot;352655552&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <div>\n        <ul class=\"nav nav-tabs\" id=\"ul352655552\"><li>\n              <a href=\"#tab352655552-0\"><i class=\"fa fa-table\"/></a>\n            </li><li>\n              <a href=\"#tab352655552-1\"><i class=\"fa fa-dot-circle-o\"/></a>\n            </li><li>\n              <a href=\"#tab352655552-2\"><i class=\"fa fa-line-chart\"/></a>\n            </li><li>\n              <a href=\"#tab352655552-3\"><i class=\"fa fa-bar-chart\"/></a>\n            </li><li>\n              <a href=\"#tab352655552-4\"><i class=\"fa fa-cubes\"/></a>\n            </li></ul>\n\n        <div class=\"tab-content\" id=\"tab352655552\"><div class=\"tab-pane\" id=\"tab352655552-0\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonc1ec31cf9c6f2ec496b6ae480d631a0d&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9},{&quot;_1&quot;:9,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;1307009599&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"_1\",\"_2\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon6f64e5b97268deb73afa594ddfb59eb7&quot;,&quot;initialValue&quot;:&quot;10&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon770c99db79e12678941a1be011904e22&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab352655552-1\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon9dc4426dfda801956ddad05fb54bd365&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9},{&quot;_1&quot;:9,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;1522037378&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/scatterChart'], \n      function(playground, _magicscatterChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicscatterChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon597a5a67f6c1d52843a1b88438932b81&quot;,&quot;initialValue&quot;:&quot;10&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon0a8f5fff0b932828bc48ff95ceebdf82&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab352655552-2\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonbece6b67e21c047b9f1ad5bfc6aa20f5&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9},{&quot;_1&quot;:9,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;1400931256&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/lineChart'], \n      function(playground, _magiclineChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiclineChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon743c8a4bdba531b6bf4388c28bf578c6&quot;,&quot;initialValue&quot;:&quot;10&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon610d89525927087f9466d77a3e451aee&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab352655552-3\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anone42afea55ae0fb921f6b14a7cd7e469b&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9},{&quot;_1&quot;:9,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;1469878960&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/barChart'], \n      function(playground, _magicbarChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicbarChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon3e881682ab652913d4d4470c2ef4146c&quot;,&quot;initialValue&quot;:&quot;10&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon8e00d7afa81533d30b72f28615f04108&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab352655552-4\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon0a20fd9769c0e9f49b41a1575063c6cb&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:2},{&quot;_1&quot;:2,&quot;_2&quot;:3},{&quot;_1&quot;:3,&quot;_2&quot;:4},{&quot;_1&quot;:4,&quot;_2&quot;:5},{&quot;_1&quot;:5,&quot;_2&quot;:6},{&quot;_1&quot;:6,&quot;_2&quot;:7},{&quot;_1&quot;:7,&quot;_2&quot;:8},{&quot;_1&quot;:8,&quot;_2&quot;:9},{&quot;_1&quot;:9,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;2042467796&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anondde4d8b0d12581e8fcb09b488a74e044&quot;,&quot;initialValue&quot;:&quot;10&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon21d5b78037db27ccf7845aa81a35fc56&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div></div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 80,
      "time" : "Took: 1 second 369 milliseconds, at 2017-5-22 19:54"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab757040890-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "9A93F6D0AB394275971A1EF8F7F39925"
    },
    "cell_type" : "code",
    "source" : "1 to 10 by 3",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res132: scala.collection.immutable.Range = Range(1, 4, 7, 10)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon13f4f58ebbe76d736909d8dd5dab569a&quot;,&quot;dataInit&quot;:[],&quot;genId&quot;:&quot;757040890&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tabs'], \n      function(playground, _magictabs) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictabs,\n    \"o\": {}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <div>\n        <ul class=\"nav nav-tabs\" id=\"ul757040890\"><li>\n              <a href=\"#tab757040890-0\"><i class=\"fa fa-table\"/></a>\n            </li><li>\n              <a href=\"#tab757040890-1\"><i class=\"fa fa-dot-circle-o\"/></a>\n            </li><li>\n              <a href=\"#tab757040890-2\"><i class=\"fa fa-line-chart\"/></a>\n            </li><li>\n              <a href=\"#tab757040890-3\"><i class=\"fa fa-bar-chart\"/></a>\n            </li><li>\n              <a href=\"#tab757040890-4\"><i class=\"fa fa-cubes\"/></a>\n            </li></ul>\n\n        <div class=\"tab-content\" id=\"tab757040890\"><div class=\"tab-pane\" id=\"tab757040890-0\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anon80b92f2a55de6883807322615195ddf7&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:4},{&quot;_1&quot;:2,&quot;_2&quot;:7},{&quot;_1&quot;:3,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;718563120&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/tableChart'], \n      function(playground, _magictableChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magictableChart,\n    \"o\": {\"headers\":[\"_1\",\"_2\"],\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon526d305af0a8118fb7abdd1ea27fcc77&quot;,&quot;initialValue&quot;:&quot;4&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonf406f103272ddbedd0bc6b59b01fd245&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab757040890-1\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anond3d509356e5a4b9e1bc38297f07911a1&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:4},{&quot;_1&quot;:2,&quot;_2&quot;:7},{&quot;_1&quot;:3,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;777627853&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/scatterChart'], \n      function(playground, _magicscatterChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicscatterChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon7a59b8f197a87ce6b849c0875c71bb15&quot;,&quot;initialValue&quot;:&quot;4&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon94ce3736f1c4689d346e3baa097dcc67&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab757040890-2\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonf440f00450aa2e626a39434596fb8637&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:4},{&quot;_1&quot;:2,&quot;_2&quot;:7},{&quot;_1&quot;:3,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;134129935&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/lineChart'], \n      function(playground, _magiclineChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magiclineChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon1439e606a14b53968f2c554dccf652b6&quot;,&quot;initialValue&quot;:&quot;4&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonf3d3fdd33ab26f1521f434dca961098f&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab757040890-3\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anoncf3b4611d6603596caf9460267aa4feb&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:4},{&quot;_1&quot;:2,&quot;_2&quot;:7},{&quot;_1&quot;:3,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;1447277162&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/barChart'], \n      function(playground, _magicbarChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicbarChart,\n    \"o\": {\"x\":\"_1\",\"y\":\"_2\",\"width\":600,\"height\":400}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon766f1e52d3e48ae0ed3162e67a66e169&quot;,&quot;initialValue&quot;:&quot;4&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonf556147b9612a63b02403ba575232e02&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div><div class=\"tab-pane\" id=\"tab757040890-4\">\n            <div>\n      <script data-this=\"{&quot;dataId&quot;:&quot;anoncef57c01c0c065333d844928cf4b1364&quot;,&quot;dataInit&quot;:[{&quot;_1&quot;:0,&quot;_2&quot;:1},{&quot;_1&quot;:1,&quot;_2&quot;:4},{&quot;_1&quot;:2,&quot;_2&quot;:7},{&quot;_1&quot;:3,&quot;_2&quot;:10}],&quot;genId&quot;:&quot;1798667820&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/playground','../javascripts/notebook/magic/pivotChart'], \n      function(playground, _magicpivotChart) {\n        // data ==> data-this (in observable.js's scopedEval) ==> this in JS => { dataId, dataInit, ... }\n        // this ==> scope (in observable.js's scopedEval) ==> this.parentElement ==> div.container below (toHtml)\n\n        playground.call(data,\n                        this\n                        ,\n                        {\n    \"f\": _magicpivotChart,\n    \"o\": {\"width\":600,\"height\":400,\"derivedAttributes\":{},\"extraOptions\":{}}\n  }\n  \n                        \n                        \n                      );\n      }\n    );/*]]>*/</script>\n    <div>\n      <span class=\"chart-total-item-count\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anon6d127ac3e6c4d8547305fc8ac80429ac&quot;,&quot;initialValue&quot;:&quot;4&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p> entries total</span>\n      <span class=\"chart-sampling-warning\"><p data-bind=\"text: value\"><script data-this=\"{&quot;valueId&quot;:&quot;anonce9ea35d1db4dc54e1efa3ed505fdc47&quot;,&quot;initialValue&quot;:&quot;&quot;}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/\nreq(\n['observable', 'knockout'],\nfunction (O, ko) {\n  ko.applyBindings({\n      value: O.makeObservable(valueId, initialValue)\n    },\n    this\n  );\n});\n        /*]]>*/</script></p></span>\n      <div>\n      </div>\n    </div></div>\n            </div></div>\n      </div>\n    </div></div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 81,
      "time" : "Took: 1 second 173 milliseconds, at 2017-5-22 19:54"
    } ]
  }, {
    "metadata" : {
      "id" : "D7A021C377D840358578268423571D28"
    },
    "cell_type" : "markdown",
    "source" : "When you need a small test data set to play with Spark, ranges can be convenient."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "14F278432C3D46ED8F3A8C282A27C71A"
    },
    "cell_type" : "code",
    "source" : "val rdd7 = sc.parallelize(1 to 50).\n    map(i => (i, i%7)).\n    groupBy{ case (i, seven) => seven }.\n    sortByKey()\nrdd7.take(7).foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "(0,CompactBuffer((7,0), (14,0), (21,0), (28,0), (35,0), (42,0), (49,0)))\n(1,CompactBuffer((1,1), (8,1), (15,1), (22,1), (29,1), (36,1), (43,1), (50,1)))\n(2,CompactBuffer((2,2), (9,2), (16,2), (23,2), (30,2), (37,2), (44,2)))\n(3,CompactBuffer((3,3), (10,3), (17,3), (24,3), (31,3), (38,3), (45,3)))\n(4,CompactBuffer((4,4), (11,4), (18,4), (25,4), (32,4), (39,4), (46,4)))\n(5,CompactBuffer((5,5), (12,5), (19,5), (26,5), (33,5), (40,5), (47,5)))\n(6,CompactBuffer((6,6), (13,6), (20,6), (27,6), (34,6), (41,6), (48,6)))\nrdd7: org.apache.spark.rdd.RDD[(Int, Iterable[(Int, Int)])] = ShuffledRDD[102] at sortByKey at <console>:72\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 82,
      "time" : "Took: 1 second 507 milliseconds, at 2017-5-22 19:54"
    } ]
  }, {
    "metadata" : {
      "id" : "CB6562E464EC41A2ABD7CDE4246BBBA4"
    },
    "cell_type" : "markdown",
    "source" : "[SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext) also has a `range` method that effectively does the same thing as `sc.parallelize(some_range)`."
  }, {
    "metadata" : {
      "id" : "797990BEF349442581344872D613911F"
    },
    "cell_type" : "markdown",
    "source" : "### Scala Interpreter (REPL) vs. Notebooks vs. Scala Compiler\n<a name=\"REPL\"></a>\nThis notebook has been using a running Scala interpreter, a.k.a. _REPL_ (\"read, eval, print, loop\") to parse the Scala code. The Spark distribution comes with a `spark-shell` script that also lets you use the interpreter from the command line, but without the nice notebook UI.\n\nIf you use `spark-shell`, there are a few other behavior changes you should know about."
  }, {
    "metadata" : {
      "id" : "E5C679DF0C204FD48041407DD1AA339A"
    },
    "cell_type" : "markdown",
    "source" : "#### Using :paste Mode\nBy default the Scala interpreter treats _each line_ you enter separately. This can cause surprises compared to how the Scala _compiler_ works, where it treats all the code in the same file in the same context.\n\nFor example, the following code, where the expression continues on the second line, is handled successfully by the compiler, but not by the interpreter.\n\n```scala\n(1 to 100)\n.map(i => i*i)\n```\n\nthe Interpreter thinks it finished parsing the expression when it hit the new line after the literal [Range](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Range), `1 to 100`. It then throws an error on the opening `.` on the next line. On the other hand, the compiler keeps compiling, ignoring the new line in this case. \n\nThis notebook also does the same thing as the \"raw\" interpreter, but in some cases, notebooks will use an interpeter command, `:paste` that tells the parser to parse all of the lines that follow together, just like the compiler would parse them, until the \"end of input\", which you indicate with `CTRL-D`. \n\nYou can't experiment with it through this notebook, but your session would look something like this:\n\n```scala\nscala> :paste\n// Entering paste mode (ctrl-D to finish)\n\n(1 to 10)\n.map(i => i*i)\n<CTRL-D>\n\n// Exiting paste mode, now interpreting.\n\nres0: scala.collection.immutable.IndexedSeq[Int] = Vector(1, 4, 9, 16, 25, 36, 49, 64, 81, 100)\n\nscala>\n```"
  }, {
    "metadata" : {
      "id" : "E214F5D1B4184A8383B4A84A823A1729"
    },
    "cell_type" : "markdown",
    "source" : "#### Ambiguities with Companion Objects\n<a name=\"Ambiguities\"></a>\nAs I wrote this notebook, I _wanted_ to demonstrate using the companion object `IIRecord` to define a method explicitly, but this leads to an ambiguity later on in the notebook if you attempt to use this method. The notebook gets confused between the case class and the object. \n\nWhile unfortunate, it's also true that once you start defining more involved case classes, with more than trivial methods and explicit additions to the default companion object, you should really define these types outside the notebook in a compiled library that you use within the notebook.\n\nThe details are beyond our scope here, but basically, you set up a project with your Scala code and build it using your favorite build tool. [SBT](http://www.scala-sbt.org/) is a popular choice for Scala, but Maven, Gradle, etc. can be used. \n\nYou want to generate a _jar_ file with the compiled artifacts, then when you start `spark-shell`, submit a Spark job with `spark-submit` or use a notebook environment like this one, you specify the jar for inclusion. For `spark-shell` and `spark-submit`, invoke it with the `--jars myproject.jar` option. For Toree with Jupyter, see the discussion on the [FAQ page](https://toree.incubator.apache.org/documentation/user/faq.html)."
  }, {
    "metadata" : {
      "id" : "309C64A9379541BEBF998A39DD9DB5E4"
    },
    "cell_type" : "markdown",
    "source" : "### Scala's Type Hierarchy\nScala's type hierarchy is similar to Java's, but with some interesting differences."
  }, {
    "metadata" : {
      "id" : "302080EB350C4C928853154BD88D660D"
    },
    "cell_type" : "markdown",
    "source" : "![Scala Type Hierarchy](http://docs.scala-lang.org/resources/images/classhierarchy.img_assist_custom.png)"
  }, {
    "metadata" : {
      "id" : "C8375B83CD0247C6BEE98E1DF8CDD0EF"
    },
    "cell_type" : "markdown",
    "source" : "In Java, all _reference types_ are descended from [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html). The name _reference type_ reflects the fact that the instances for all these types are allocated on the _heap_ and program variables are references to those heap locations.\n\nThe primitives types, `int`, `long`, etc. are not considered part of the type hierarchy and are treated specially. This is in part a performance optimization, as instances of these types fit in CPU registers and the values are pushed onto stack frames. However, they have wrapper or \"boxed\" types, `Integer`, `Long`, etc., that are part of the type hierarchy, which you must use with Java's collections, for example (with the exception of arrays).\n\nInstead, Scala treats the primitives at the code level as basically the same as the reference types. You don't use `new Int(100)` for example, but you can call methods on `Int` instances. The code generated, in most cases, uses the optimized JVM primitives. \n\nHence, the Scala type hierarchy defines a type [Any](http://www.scala-lang.org/api/current/#scala.Any) to be the a parent type of _both_ reference types and \"value\" types (for the primitives). Each of those subhierarchies have parent types, [AnyRef](http://www.scala-lang.org/api/current/#scala.AnyRef) is effectively the same as [java.lang.Object](https://docs.oracle.com/javase/8/docs/api/java/lang/Object.html), and [AnyVal](http://www.scala-lang.org/api/current/#scala.AnyVal) is the parent of the value types.\n\nFinally, for better \"soundness\", the Scala type system defines a real type to represent [Null](http://www.scala-lang.org/api/current/#scala.Null) and [Nothing](http://www.scala-lang.org/api/current/#scala.Nothing). By defining `Null` to be the subtype of all reference types `AnyRefs` (but not `AnyVals`), it supports at the type level the (unfortunate) practice of using `null` for a reference value.\n\nHowever, `null` is not allowed for an `AnyVal`, so the true \"bottom type\" of the hierarchy is `Nothing`. Why is that useful. I'll explain in the next section."
  }, {
    "metadata" : {
      "id" : "5D85EDA6B1E340D49CA04EB1B8379A1D"
    },
    "cell_type" : "markdown",
    "source" : "<a name=\"TryOptionNull\"></a>\n### Try vs. Option vs. null\n\nRecall the signature of our `curl` method near the beginning of this notebook:\n\n```scala\ndef curl(sourceURLString: String, targetDirectoryString: String): File = ...\n```\n\nIt returns a `File` when everything goes well, but it could throw an exception. An alternative is return a `Try[File]`, where the [Try](http://www.scala-lang.org/api/current/index.html#scala.util.Try) encapsulates both cases in the return value, as we'll discuss next. We'll also discuss an alternative, [Option](http://www.scala-lang.org/api/current/index.html#scala.Option)."
  }, {
    "metadata" : {
      "id" : "EA03F2FA02244AF989A920AC520BA08E"
    },
    "cell_type" : "markdown",
    "source" : "Suppose instead that we declared `curl` to return [util.Try[File]](http://www.scala-lang.org/api/current/index.html#scala.util.Try). The only change to the body would be to simply add `Try` before the opening bracket:\n\n```\ndef curl(sourceURLString: String, targetDirectoryString: String): Try[File] = Try {...}\n```\n\nNow, the reader knows from the method signature that it might fail somehow. If a call fails, the relevant exception will be returned wrapped in a subclass of `Try`, called [util.Failure[File]](http://www.scala-lang.org/api/current/index.html#scala.util.Failure). However, if `curl` succeeds, the `File` will be returned wrapped in the other subclass of `Try`, [util.Success[File]](http://www.scala-lang.org/api/current/index.html#scala.util.Success).\n\nBecause of Scala's type safety, the caller of `curl` would have to determine if a `Success` or `Failure` was returned and handle it appropriately.\n\nScala does not have methods declare the exceptions they will throw using the `throws` clause, like Java. So, looking at the signature of our original version, there's no obvious way to know if it throws an exception _or_ returns `null` on failure:\n```scala\ndef curl(sourceURLString: String, targetDirectoryString: String): File = {...}\n```\n\nIf we choose to catch exceptions internally and return `null`, the caller has to remember to check for `null`. Otherwise, the infamous [NullPointerException](https://docs.oracle.com/javase/8/docs/api/java/lang/NullPointerException.html) might happen occasionally if the caller assumes a non-`null` value is returned. So, using `Try[File]` prevents us from this loophole. _It helps the user do the right thing!_\n\nAlso, using `Try` rather than simply throwing an exception, means that `curl` always returns \"normally\", so the caller maintains full control of the call stack and special exception-catching logic isn't required (although you can do that in Scala, if you want to)."
  }, {
    "metadata" : {
      "id" : "02910EB617EE423582DC23CAB4FBE5F6"
    },
    "cell_type" : "markdown",
    "source" : "What are all the possible valid subclasses of `Try`? Really, there are only two, `Success` and `Failure`. It would be a mistake to allow a user to define other subtypes, like `MaybeCouldFailButWhoKnows`, because users of `Try` in pattern matching will always want to know that there are only two possibilities. Scala adds a keyword to enforce this logical behavior. `Try` is actually declared as follows:\n\n```scala\nsealed abstract class Try[+T] extends AnyRef\n```\n\n(`AnyRef` is the same as Java's `Object` supertype.) The `sealed` keyword says that _no_ subclasses of `Try` can be declared, _except_ in the same source file (which the library author wrote). Hence, users of `Try` can't declare their own subclasses, subverting the logical structure of this type hierarchy and other user's code that relies on this structure."
  }, {
    "metadata" : {
      "id" : "0CA5438E96E34CBC860C65C61DF2B2B8"
    },
    "cell_type" : "markdown",
    "source" : "What if we have a situation where it makes no sense to involve an exception, but we want similar logic to handle the case where I have something or I don't (put another way, I have either 0 or 1 elements)? This is where [Option[T]](http://www.scala-lang.org/api/current/index.html#scala.Option) comes in. \n\n`Option` is analogous to `Try`, it is a `sealed` abstract type with two possible subtypes:\n\n* [Some[T]](http://www.scala-lang.org/api/current/index.html#scala.None): I have a an instance of `T` for you, inside the `Some[T]`.\n* [None](http://www.scala-lang.org/api/current/index.html#scala.None): I don't have a value for you, sorry.\n\nNote that a hash map is a great example where I either have a value for a given key or I don't. Therefore, for Scala's [Map[K,V]](http://www.scala-lang.org/api/current/index.html#scala.collection.Map) abstraction, where `K` is the key type and `V` is the value type, the `get` method has this signature:\n\n```scala\ndef get(key: K): Option[V]\n```\n\nOnce again, you know from the type signature that you may or may not get a value instance for the input key, _and_ you **must** determine whether you got a `Some[V]` or a `None` as the result. By never returning a `null` value, we remove the risk of a `NullPointerException` if the caller forgets to check for it!"
  }, {
    "metadata" : {
      "id" : "000CD7F1DA0647BBBC49DD21B4A5A265"
    },
    "cell_type" : "markdown",
    "source" : "So, how do we determine which `Option[T]` was returned? Let's look a few examples using `Option`. Can you guess what they are doing? Check the [Option Scaladocs](http://www.scala-lang.org/api/current/#scala.Option) to confirm. `Try` can be used similarly, with a few other ways available that we won't discuss here (but see the [Try Scaladocs](http://www.scala-lang.org/api/current/#scala.util.Try))."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab643049376-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "417601022616427EA8B7ED8C982C6239"
    },
    "cell_type" : "code",
    "source" : "val options = Seq(None, Some(2), Some(3), None, Some(5))\n\nval numbers1 = options.map { o =>\n    o.getOrElse(-1)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "options: Seq[Option[Int]] = List(None, Some(2), Some(3), None, Some(5))\nnumbers1: Seq[Int] = List(-1, 2, 3, -1, 5)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 83,
      "time" : "Took: 795 milliseconds, at 2017-5-22 19:55"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "presentation" : {
        "tabs_state" : "{\n  \"tab_id\": \"#tab470759055-0\"\n}",
        "pivot_chart_state" : "{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"
      },
      "id" : "563BAB96709E407C87098F477544F6E1"
    },
    "cell_type" : "code",
    "source" : "val numbers2 = options.map {\n    case None    => -1\n    case Some(i) => i  // Note how we extract the enclosed value.\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "numbers2: Seq[Int] = List(-1, 2, 3, -1, 5)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 84,
      "time" : "Took: 649 milliseconds, at 2017-5-22 19:55"
    } ]
  }, {
    "metadata" : {
      "id" : "01C6A19D2CDF490B8D9A776894CD4C0A"
    },
    "cell_type" : "markdown",
    "source" : "If you just want to ignore the `None` values, use a _for comprehension_. We could print them as before, but this time we'll `yield` each value, constructing a new `Seq` (sequence) of numbers."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0E1BF86C497B43568BC1A7B3B2E92FB5"
    },
    "cell_type" : "code",
    "source" : "val numbers = for {\n    opt   <- options  // loop through the options, assign each to \"option\"\n    value <- opt      // extract the value from the Some, or if None, skip to the next loop\n} yield value",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "numbers: Seq[Int] = List(2, 3, 5)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 85,
      "time" : "Took: 764 milliseconds, at 2017-5-22 19:56"
    } ]
  }, {
    "metadata" : {
      "id" : "183B495289174E1A9D95EEBBBB5C505D"
    },
    "cell_type" : "markdown",
    "source" : "Finally, you might wonder how `None` is declared. Consider this example:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "01EA7C7FFFBE4469A0BDF9D87F283162"
    },
    "cell_type" : "code",
    "source" : "val opts: Seq[Option[String]] = Seq(Some(\"hello\"), None, Some(\"world!\"))\nopts.foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Some(hello)\nNone\nSome(world!)\nopts: Seq[Option[String]] = List(Some(hello), None, Some(world!))\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 86,
      "time" : "Took: 997 milliseconds, at 2017-5-22 19:56"
    } ]
  }, {
    "metadata" : {
      "id" : "041002EBDA9849ADB9D5098ED8A4EEF0"
    },
    "cell_type" : "markdown",
    "source" : "This works, so it must mean that `None` is a valid subclass of `Option[String]`. That's actually true for all `Option[T]`. How can a single object be a valid subtype for _all_ of them? Here is how it's declared (omitting some details):\n\n```scala\nobject None extends Option[Nothing] {...}\n\n```\n\n`None` carries no \"state\" information, because it doesn't wrap an instance like `Some[T]` does. Hence, we only need one instance for all uses, so it's declared as an object. Recall we mentioned above that the type system has a [Nothing](http://www.scala-lang.org/api/current/#scala.Nothing) type, which is a subtype of all other types. Without diving into too many details, if a variable is of type `Option[String]`, then you can use an `Option[Nothing]` for it (i.e., the latter is a subtype of the former). This is why `Nothing` is useful, for cases like `None`, so we can have one instance of it, but still obey the rules of Scala's object-oriented type system."
  }, {
    "metadata" : {
      "id" : "F39CA8B00D7F41C9851C49E0E0D7D5A1"
    },
    "cell_type" : "markdown",
    "source" : "### Implicits\n<a name=\"implicits\"></a>\nScala has a powerful mechanism known as _implicits_ that is used in the Spark Scala API. Implicits are a big topic, so we'll focus just on the uses of it that are most important to understand.  "
  }, {
    "metadata" : {
      "id" : "6C195C1AEB8E471699BB6B4D8541598F"
    },
    "cell_type" : "markdown",
    "source" : "#### Type Conversions\nWe used `RDD` methods like `reduceByKey` above, but if you search for this method in the [RDD Scaladoc page](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD), you won't find it. Instead it's defined in the [PairRDDFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) type (along with all the other `*ByKey` methods). So, how can we use these methods as if they are defined for `RDD`??\n\nWhen the Scala compiler sees code calling a method that doesn't exist on the type, it looks for an _implicit conversion_ in the current scope, which can transform the instance into another type (i.e., by wrapping it), where the other type provides the needed method. The full signature inferred for the method as it's used must match the definition in the wrapping class.\n\n> **Note:** If you don't find a method in the [Spark Scaladocs](http://spark.apache.org/docs/latest/api/scala/index.html#package) for a type where you think it should be defined, look for related helper types with the method.\n\nHere's a small Scala example of how this works:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "9CE1F09078A64727BEE4A59B32F7CC19"
    },
    "cell_type" : "code",
    "source" : "// A sample class. Note it doesn't define a `toJSON` method:\ncase class Person(name: String, age: Int = 0)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined class Person\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 87,
      "time" : "Took: 845 milliseconds, at 2017-5-22 19:57"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "41EBF853BA8E4D2B8C39B66897E45A23"
    },
    "cell_type" : "code",
    "source" : "// To scope them, define implicit conversions within an object\nobject implicits {\n\n    // `implicit` keyword tells the compiler to consider this conversion.\n    // It takes a `Person`, returning a new instance of `PersonToJSONString`,\n    // then resolves the invocation of `toJSON`.\n    implicit class PersonToJSONString(person: Person) {\n        def toJSON: String = s\"\"\"{\"name\": ${person.name}, \"age\": ${person.age}}\"\"\"\n    }\n}\n\nimport implicits._        // Now it is visible in the current scope.\n\nval p = Person(\"Dean Wampler\", 39)\n\n// Magic conversion to `PersonToJSONString`, then `toJSON` is called.\np.toJSON",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined object implicits\nimport implicits._\np: Person = Person(Dean Wampler,39)\nres142: String = {\"name\": Dean Wampler, \"age\": 39}\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 88,
      "time" : "Took: 1 second 716 milliseconds, at 2017-5-22 19:57"
    } ]
  }, {
    "metadata" : {
      "id" : "66E48B7868F94F9383CF62A0BB79F2D0"
    },
    "cell_type" : "markdown",
    "source" : "For `RDDs`, the implicit conversions to `PairRDDFunctions` and other support types are handled for you. Similarly, some conversions are used by Spark SQL."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "CC6CE2FC418846DE8E5133836A8AC66C"
    },
    "cell_type" : "code",
    "source" : "val wtc = iiDF.select($\"word\", $\"total_count\")\nwtc.show",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+-----------+-----------+\n|       word|total_count|\n+-----------+-----------+\n|          a|       3350|\n|    abandon|          6|\n|      abate|          3|\n|  abatement|          1|\n|     abbess|          8|\n|      abbey|          9|\n|abbominable|          1|\n|abbreviated|          1|\n|       abed|          2|\n|   abetting|          1|\n|abhominable|          1|\n|      abhor|          5|\n|     abhors|          2|\n|      abide|          5|\n|     abides|          1|\n|    ability|          2|\n|     abject|          2|\n|     abjure|          1|\n|    abjured|          2|\n|       able|          9|\n+-----------+-----------+\nonly showing top 20 rows\n\nwtc: org.apache.spark.sql.DataFrame = [word: string, total_count: int]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 89,
      "time" : "Took: 1 second 380 milliseconds, at 2017-5-22 19:57"
    } ]
  }, {
    "metadata" : {
      "id" : "EC8352A276FA495EB26F84C4B4402E0B"
    },
    "cell_type" : "markdown",
    "source" : "The column-reference syntax `$\"name\"` is implemented using the same mechanism in the Scala library that implements interpolated strings, `s\"$foo\"`. The `import sqlc.implicits._` makes it available. \n\nNote we imported something from an _instance_, rather than a package or type, as allowed in Java. This can be a useful feature in Scala, but it's also fragile, If you try `import sqlContext.implicits._`, you'll get a compiler error that a \"stable identifier\" is required. It turns out that doing the value assignment, `val sqlc = sqlContext` first meets this requirement. This is unique to the notebook environment. You normally won't see this problem if you use the `spark-shell` that comes with a Spark distribution or you write a Spark program and compile it with the Scala compiler.\n\nHowever, it would be better if Spark defined this `implicits` object on the `SQLContext` companion object instead of on instances of it!"
  }, {
    "metadata" : {
      "id" : "AD9665EA860E4C2F8CC2E408C20603F3"
    },
    "cell_type" : "markdown",
    "source" : "For completeness, but unrelated to implicits, the `DataFrame` API lets you write SQL-like queries with a programmatic API. If you want to use built in functions like `min`, `max`, etc. on columns, you need the following `import` statement:"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "6F83A0CC81934708ACD1E136CE4F0C1A"
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.sql.functions._",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.sql.functions._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 90,
      "time" : "Took: 754 milliseconds, at 2017-5-22 19:57"
    } ]
  }, {
    "metadata" : {
      "id" : "C6C9D19C7E9A41D28CECF6E8790D5646"
    },
    "cell_type" : "markdown",
    "source" : "Now we can use `min`, `max`, `avg`, etc."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "B2BD1F33FB2E47708D33564A398AA7F1"
    },
    "cell_type" : "code",
    "source" : "val mma = iiDF.select(min(\"total_count\"), max(\"total_count\"), avg(\"total_count\"))\nmma.show",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------------+----------------+------------------+\n|min(total_count)|max(total_count)|  avg(total_count)|\n+----------------+----------------+------------------+\n|               1|            5208|16.651743683350947|\n+----------------+----------------+------------------+\n\nmma: org.apache.spark.sql.DataFrame = [min(total_count): int, max(total_count): int ... 1 more field]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 91,
      "time" : "Took: 1 second 693 milliseconds, at 2017-5-22 19:57"
    } ]
  }, {
    "metadata" : {
      "id" : "E2ED40DF8736441589E4366F32AE84F8"
    },
    "cell_type" : "markdown",
    "source" : "#### Implicit Method Arguments\nOne other use of implicits worth understanding is _implicit arguments_ to methods. You will encounter this mechanism used when you read the Spark Scaladocs, even though you might never realize you're actually using it in your code!\n\nRecall I mentioned previously that you can define default values for method arguments. I just used it for the `age` argument for `Person`:\n\n```scala\ncase class Person(name: String, age: Int = 0)\n```\n\nSometimes we need something more sophisticated. For example, our library might have a group of methods that need a special argument passed to them that provides useful \"context\" information, but you don't want the user to be required to explicitly pass this argument every time. Other times you might use implicit arguments to make the API \"cleaner\", but still have some control over what's allowed.\n\nHere's an example, that's partly inspired by Scala's [Seq.sum](http://www.scala-lang.org/api/current/#scala.collection.Seq) method. Wouldn't it be great if I happen to have a collection of things I can \"add\" together, if I could just call `sum` on the collection? Let's do this in a slightly different way, with a helper `sum` method outside of `Seq`."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "56FAB05720ED42D78975EA9145F27795"
    },
    "cell_type" : "code",
    "source" : "trait Add[T] {\n    def add(t1: T, t2: T): T\n}\n\n// Nested implicits so they don't conflict with the previous object implicits.\nobject Adder {\n    object implicits {\n        implicit val intAdd = new Add[Int] { \n            def add(i1: Int, i2: Int): Int = i1+i2 \n        }\n        implicit val doubleAdd = new Add[Double] { \n            def add(d1: Double, d2: Double): Double = d1+d2 \n        }\n        implicit val stringAdd = new Add[String] { \n            def add(s1: String, s2: String): String = s1+s2 \n        }\n        // etc...\n    }\n}\n\nimport Adder.implicits._\n\n// NOTE: TWO argument lists!\ndef sum[T](ts: Seq[T])(implicit adder: Add[T]): T = {\n    ts.reduceLeft((t1, t2) => adder.add(t1, t2))\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined trait Add\ndefined object Adder\nimport Adder.implicits._\nsum: [T](ts: Seq[T])(implicit adder: Add[T])T\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 92,
      "time" : "Took: 2 seconds 145 milliseconds, at 2017-5-22 19:57"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "481097DD68F447E2864FBB4F59EDB8CD"
    },
    "cell_type" : "code",
    "source" : "sum(0 to 10)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res150: Int = 55\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "55"
      },
      "output_type" : "execute_result",
      "execution_count" : 93,
      "time" : "Took: 1 second 753 milliseconds, at 2017-5-22 19:57"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "0AC34A98556444D3ACFFEE51C4685D24"
    },
    "cell_type" : "code",
    "source" : "sum(0.0 to 5.5 by 0.3)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res152: Double = 51.29999999999999\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "51.29999999999999"
      },
      "output_type" : "execute_result",
      "execution_count" : 94,
      "time" : "Took: 1 second 274 milliseconds, at 2017-5-22 19:57"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "282E7CEDA0C645AB8086D547A957B6FC"
    },
    "cell_type" : "code",
    "source" : "sum(Seq(\"one\", \"two\", \"three\"))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res154: String = onetwothree\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "onetwothree"
      },
      "output_type" : "execute_result",
      "execution_count" : 95,
      "time" : "Took: 1 second 356 milliseconds, at 2017-5-22 19:58"
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "66BA8398396D47B3B7F6432596396F8E"
    },
    "cell_type" : "code",
    "source" : "// Will fail, because there's no Add[Char] in scope:\nsum(Seq('a', 'b', 'c'))   // Characters",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "<console>:89: error: could not find implicit value for parameter adder: Add[Char]\n       sum(Seq('a', 'b', 'c'))   // Characters\n          ^\n"
    } ]
  }, {
    "metadata" : {
      "id" : "AA7F3CE084AF4EAAADFD1B9460750DF2"
    },
    "cell_type" : "markdown",
    "source" : "So, the implicit values `intAdd`, `doubleAdd`, and `stringAdd`, were used by the Scala interpreter for the `adder` argument in the second _argument list_ for `sum`. Note that you have to use a second argument list and all arguments there must be implicit. \n\nWe could have avoided using implicit arguments if we defined custom `sum` methods for every type. That would have been simpler in this trivial case, but for nontrivial methods, the duplication is worth avoiding. Another advantage of this mechanism is that the user can define her own implicit `Add[T]` instances for domain types (say for example, `Money`) and they would \"just work\".\n\nThe Scala collections API uses this mechanism to know how to construct a new collection of the same kind as the input collection when you use `map`, `flatMap`, `reduceLeft`, etc.\n\nSpark uses this pattern for [Encoders](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder) in  Spark SQL. Encoders are used to serialize values into the new, compact memory encoding introduced in the _Tungsten_ project (see for example, [here](https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/)). Here's an example of creating a [Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset), where the `toDS` method is first \"added\" to a Scala [Seq](http://www.scala-lang.org/api/current/#scala.collection.Seq) through an implicit conversion (specifically [SQLImplicits.localSeqToDatasetHolder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLImplicits), which is brought into scope by the `import sqlc.implicits._` statement earlier) and then `toDS` uses `Encoders` internally."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "934904B8DC224EEC83154CD6149F3AB7"
    },
    "cell_type" : "code",
    "source" : "(0 to 10).toDS()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res158: org.apache.spark.sql.Dataset[Int] = [value: int]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "<div class=\"df-canvas\">\n      <script data-this=\"{&quot;dataId&quot;:&quot;anonb2419541b11184c5e6bf874e53b55c4a&quot;,&quot;partitionIndexId&quot;:&quot;anona057c3e62755c24481fcdcb389ca7e00&quot;,&quot;numPartitions&quot;:1,&quot;dfSchema&quot;:{&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;value&quot;,&quot;type&quot;:&quot;integer&quot;,&quot;nullable&quot;:false,&quot;metadata&quot;:{}}]}}\" type=\"text/x-scoped-javascript\">/*<![CDATA[*/req(['../javascripts/notebook/dataframe','../javascripts/notebook/consoleDir'], \n      function(dataframe, extension) {\n        dataframe.call(data, this, extension);\n      }\n    );/*]]>*/</script>\n      <link rel=\"stylesheet\" href=\"/assets/stylesheets/ipython/css/dataframe.css\" type=\"text/css\"/>\n    </div>"
      },
      "output_type" : "execute_result",
      "execution_count" : 97,
      "time" : "Took: 1 second 313 milliseconds, at 2017-5-22 19:58"
    } ]
  }, {
    "metadata" : {
      "id" : "A37856B8529E4A5D86C787C595E14FCA"
    },
    "cell_type" : "markdown",
    "source" : "# Conclusions\nI appreciate the effort you put into studying this notebook. I hope you enjoyed it as much as I enjoyed writing it. Please post issues on how I can improve it to the [GitHub repo](https://github.com/deanwampler/JustEnoughScalaForSpark).\n\nNow you know the core elements of Scala that you need for using the Spark Scala API. I hope you can appreciate the power and elegance of Scala. I hope you will choose to use it for all of your data engineering tasks, not just for Spark. \n\nWhat about data science? There are many people who use Scala for data science in Spark, but today Python and R have much richer libraries for Mathematics and Machine Learning. That will change over time, but for now, you'll need to decide which language best fits your needs.\n\nAs you use Scala, there will be more things you'll want to understand that we haven't covered, including common idioms, conventions, and tools used in the Scala community. The references at the beginning of the notebook will give you the information you need.\n\nBest wishes.\n\n[Dean Wampler](mailto:deanwampler@gmail.com)<br/>\n[@deanwampler](http://twitter.com/deanwampler)"
  }, {
    "metadata" : {
      "id" : "F824A7CAA46A49BD83A19E9E06399954"
    },
    "cell_type" : "markdown",
    "source" : "## Appendix: Exercise Solutions\n<a name=\"ExerciseSolutions\"></a>\nLet's discuss the solutions to exercises that weren't already solved earlier in the notebook."
  }, {
    "metadata" : {
      "id" : "6D9D398315014C05822F8889BBDA49C2"
    },
    "cell_type" : "markdown",
    "source" : "### Filter for Plays that Have \"of\" in the Name\nYou can add the condition (comment `// <== here`) immediate after defining `play`. You could do it later, after either of the subsequent two expressions, but then you're doing needless computation. Change `true` to `false` to print plays that don't contain \"of\"."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "8E63870D6F26419BB1B375463C69009D"
    },
    "cell_type" : "code",
    "source" : "val list2 = for {\n    play <- plays \n    if (play.contains(\"of\") == true)                            // <== here\n    playFileString = targetDirName + pathSeparator + play\n    playFile = new File(playFileString)\n} yield {\n    val successString = if (playFile.exists) \"Success!\" else \"NOT FOUND!!\"\n    s\"$playFileString\\t$successString\"\n}\nlist2.foreach(println)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "data/shakespeare/tamingoftheshrew\tSuccess!\ndata/shakespeare/comedyoferrors\tSuccess!\ndata/shakespeare/merrywivesofwindsor\tSuccess!\nlist2: Seq[String] = List(data/shakespeare/tamingoftheshrew\tSuccess!, data/shakespeare/comedyoferrors\tSuccess!, data/shakespeare/merrywivesofwindsor\tSuccess!)\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 98,
      "time" : "Took: 1 second 162 milliseconds, at 2017-5-22 19:58"
    } ]
  }, {
    "metadata" : {
      "id" : "58DE43817782420189CF9BA4DD12AE5C"
    },
    "cell_type" : "markdown",
    "source" : "### More Queries"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "D69DA27B224B40C0A070D64DF221A95D"
    },
    "cell_type" : "code",
    "source" : "val topLocationsLoveHate = sparkSession.sql(\"\"\"\n    SELECT word, total_count, locations[0] AS top_location, counts[0] AS top_count\n    FROM inverted_index \n    WHERE word LIKE 'love%' OR word LIKE 'unlove%' OR word LIKE 'hate%'\n\"\"\")\ntopLocationsLoveHate.show(40)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+-------+-----------+--------------------+---------+\n|   word|total_count|        top_location|top_count|\n+-------+-----------+--------------------+---------+\n|   hate|         22|midsummersnightsd...|        9|\n|  hated|          6|midsummersnightsd...|        4|\n|hateful|          5|midsummersnightsd...|        3|\n|  hates|          5|         asyoulikeit|        2|\n| hateth|          1|midsummersnightsd...|        1|\n|   love|        662|    loveslabourslost|      121|\n|  loved|         38|         asyoulikeit|       13|\n| lovely|         15|midsummersnightsd...|        7|\n|  lover|         33|         asyoulikeit|       14|\n| lovers|         31|midsummersnightsd...|       17|\n|  loves|         51| muchadoaboutnothing|       10|\n| lovest|          8|    tamingoftheshrew|        3|\n| loveth|          2|    loveslabourslost|        1|\n|unloved|          1|midsummersnightsd...|        1|\n+-------+-----------+--------------------+---------+\n\ntopLocationsLoveHate: org.apache.spark.sql.DataFrame = [word: string, total_count: int ... 2 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 99,
      "time" : "Took: 1 second 754 milliseconds, at 2017-5-22 19:58"
    } ]
  }, {
    "metadata" : {
      "id" : "8ADC76DC45314B31B2D085B78698FE53"
    },
    "cell_type" : "markdown",
    "source" : "### Return the Top Two Locations and Counts\nWe used the `DataFrame` API to write a SQL query that returned the top location and count. Adding the next one is straightforward. What do you observe is returned when there isn't a second location and count?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "83520E5410494DA79BE37C05D8F63A57"
    },
    "cell_type" : "code",
    "source" : "val topTwoLocations = sparkSession.sql(\"\"\"\n    SELECT word, total_count, \n        locations[0] AS first_location,  counts[0] AS first_count,\n        locations[1] AS second_location, counts[1] AS second_count\n    FROM inverted_index \n    WHERE word LIKE '%love%' OR word LIKE '%hate%'\n\"\"\")\ntopTwoLocations.show(100)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+--------+-----------+--------------------+-----------+--------------------+------------+\n|    word|total_count|      first_location|first_count|     second_location|second_count|\n+--------+-----------+--------------------+-----------+--------------------+------------+\n| beloved|         11|    tamingoftheshrew|          4|         asyoulikeit|           3|\n|  cloven|          1|    loveslabourslost|          1|                null|        null|\n|  cloves|          1|    loveslabourslost|          1|                null|        null|\n|   glove|          3|    loveslabourslost|          2|        twelfthnight|           1|\n|  glover|          1| merrywivesofwindsor|          1|                null|        null|\n|  gloves|          5| merrywivesofwindsor|          3|         asyoulikeit|           1|\n|    hate|         22|midsummersnightsd...|          9|         asyoulikeit|           6|\n|   hated|          6|midsummersnightsd...|          4|         asyoulikeit|           2|\n| hateful|          5|midsummersnightsd...|          3|    loveslabourslost|           1|\n|   hates|          5|         asyoulikeit|          2| merrywivesofwindsor|           1|\n|  hateth|          1|midsummersnightsd...|          1|                null|        null|\n|    love|        662|    loveslabourslost|        121|         asyoulikeit|         119|\n|   loved|         38|         asyoulikeit|         13| muchadoaboutnothing|          13|\n|  lovely|         15|midsummersnightsd...|          7|    tamingoftheshrew|           5|\n|   lover|         33|         asyoulikeit|         14|midsummersnightsd...|          10|\n|  lovers|         31|midsummersnightsd...|         17|         asyoulikeit|           6|\n|   loves|         51| muchadoaboutnothing|         10| merrywivesofwindsor|           9|\n|  lovest|          8|    tamingoftheshrew|          3| muchadoaboutnothing|           2|\n|  loveth|          2|    loveslabourslost|          1|    tamingoftheshrew|           1|\n| unloved|          1|midsummersnightsd...|          1|                null|        null|\n|   whate|          4|    tamingoftheshrew|          3|         asyoulikeit|           1|\n|whatever|          1|    tamingoftheshrew|          1|                null|        null|\n+--------+-----------+--------------------+-----------+--------------------+------------+\n\ntopTwoLocations: org.apache.spark.sql.DataFrame = [word: string, total_count: int ... 4 more fields]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 100,
      "time" : "Took: 2 seconds 345 milliseconds, at 2017-5-22 19:59"
    } ]
  }, {
    "metadata" : {
      "id" : "A12E1200260C4F43890B3B7C9F8801EF"
    },
    "cell_type" : "markdown",
    "source" : "### Removing Stop Words\nRecall you were asked to implement a `keep(word: String):Boolean` method that filters stop words.\n\nFirst, let's implement `keep`. You can find lists of stop words on the web. One such list for English can be found [here]( * From http://norm.al/2009/04/14/list-of-english-stop-words/). It includes many words that you might not consider stop words. Nevertheless, I'll just use a smaller list here.\n\nNote that I'll use a Scala [Set](http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.Set) to hold the stop words. We want _O(1)_ look-up performance. We just want to know if the word is in the set or not.\n\nI'll also add \"\", so I can remove the explicit test for it.\n\nFinally, we'll embed the whole thing in a new Scala `object`. This extra encapsulation is a way to work around occasional problems with \"task not serializable\" errors."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false,
      "id" : "1E1181BF7B5D43B28638787497FE37B1"
    },
    "cell_type" : "code",
    "source" : "object StopWords {\n  val stopWords = sc.broadcast(Set(\"\", \"a\", \"an\", \"and\", \"I\", \"he\", \"she\", \"it\", \"the\"))\n\n  def keep(word: String): Boolean = {\n    word.size > 0 && (stopWords.value.contains(word) == false)\n  }\n\n  def iiStop(sc: SparkContext) = sc.wholeTextFiles(shakespeare.toString).\n        flatMap {\n            case (location, contents) => \n                val words = contents.split(\"\"\"\\W+\"\"\").\n                    map(word => word.toLowerCase).  // Do this early, before keep(), because it only looks for lower case words.\n                    filter(word => keep(word))      // <== filter here\n                val fileName = location.split(java.io.File.separator).last\n                words.map(word => ((word, fileName), 1))\n        }.\n        reduceByKey((count1, count2) => count1 + count2).\n        map { \n            case ((word, fileName), count) => (word, (fileName, count)) \n        }.\n        groupByKey.\n        sortByKey(ascending = true).\n        map { \n            case (word, iterable) => \n                val vect = iterable.toVector.sortBy { \n                    case (fileName, count) => (-count, fileName) \n                }\n                val (locations, counts) = vect.unzip  \n                val totalCount = counts.reduceLeft((n1,n2) => n1+n2)        \n                (word, totalCount, locations, counts)\n        }\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined object StopWords\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 101,
      "time" : "Took: 1 second 5 milliseconds, at 2017-5-22 19:59"
    } ]
  }, {
    "metadata" : {
      "id" : "D501D924C0C94FED82D4B7A85D83D4D8"
    },
    "cell_type" : "markdown",
    "source" : "> **NOTE:** If the following cell _still_ throws a \"Task not serializable\" exception, it's a due to an ambiguity in the Scala interpreter. The code will work fine in a standalone Spark program and may work better in a shorter notebook!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true,
      "id" : "F44D9D36301344779B87054C92F313EB"
    },
    "cell_type" : "code",
    "source" : "StopWords.iiStop(sc).take(100).foreach(println)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "id" : "E1FDAF1071864528A32C3CDC0BE0F5E4"
    },
    "cell_type" : "markdown",
    "source" : "One last thing, we now have `filter(word => keep(word))`, but note how we used `println` in the previous cell to see results. We can do something similar with `filter` and instead write `filter(keep)`. \n\nWhat does this mean exactly? It tells the compiler \"convert the _method_ `keep` to a _function_ and pass that to `filter`.\" This works because `keep` already does what `filter` wants, take a single string argument and return a boolean result.\n\nPassing `keep` is actually different than passing `word => keep(word)`, which is an _anonymous_ function that _calls_ keep. We are using `keep` as the function itself, rather than constructing a function that uses `keep`."
  } ],
  "nbformat" : 4
}